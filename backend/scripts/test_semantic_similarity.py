#!/usr/bin/env python3
"""
æ¸¬è©¦èªç¾©ç›¸ä¼¼åº¦æª¢æ¸¬åŠŸèƒ½

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨èªç¾©ç›¸ä¼¼åº¦æœå‹™é€²è¡Œï¼š
1. ç”Ÿæˆå’Œå­˜å„²å‘é‡åµŒå…¥
2. æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« 
3. æª¢æ¸¬é‡è¤‡å…§å®¹
4. åŸ·è¡Œç›¸ä¼¼åº¦æœç´¢
"""

import asyncio
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import select  # noqa: E402

from src.config.database import get_db_config  # noqa: E402
from src.models.article import Article  # noqa: E402
from src.services.semantic_similarity import get_semantic_service  # noqa: E402


async def test_basic_embedding():
    """æ¸¬è©¦åŸºæœ¬çš„åµŒå…¥ç”ŸæˆåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ“ æ¸¬è©¦ 1: ç”Ÿæˆæ–‡æœ¬åµŒå…¥")
    print("="*60)

    service = get_semantic_service()

    # æ¸¬è©¦æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹è®Šä¸–ç•Œï¼Œæ©Ÿå™¨å­¸ç¿’æŠ€è¡“ä¸æ–·é€²æ­¥",
        "AI and machine learning are transforming the world",
        "ä»Šå¤©å¤©æ°£å¾ˆå¥½ï¼Œé©åˆå‡ºå»æ•£æ­¥",
        "æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç¶“ç¶²çµ¡"
    ]

    embeddings = []
    for text in test_texts:
        print(f"\nç”ŸæˆåµŒå…¥: {text[:50]}...")
        try:
            embedding = await service.generate_embedding(text)
            embeddings.append(embedding)
            print(f"âœ… æˆåŠŸï¼ç¶­åº¦: {len(embedding)}")
        except Exception as e:
            print(f"âŒ å¤±æ•—: {e}")
            embeddings.append(None)

    # è¨ˆç®—ç›¸ä¼¼åº¦
    if all(embeddings):
        print("\nğŸ“Š æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é™£:")
        print("-" * 40)

        from sklearn.metrics.pairwise import cosine_similarity

        similarity_matrix = cosine_similarity(embeddings)

        for i, text1 in enumerate(test_texts):
            print(f"\næ–‡æœ¬ {i+1}: {text1[:30]}...")
            for j, _text2 in enumerate(test_texts):
                if i != j:
                    similarity = similarity_matrix[i][j]
                    print(f"  èˆ‡æ–‡æœ¬ {j+1} ç›¸ä¼¼åº¦: {similarity:.2%}")

    return embeddings[0] if embeddings else None


async def test_article_operations():
    """æ¸¬è©¦æ–‡ç« ç›¸é—œæ“ä½œ"""
    print("\n" + "="*60)
    print("ğŸ“š æ¸¬è©¦ 2: æ–‡ç« åµŒå…¥å­˜å„²å’Œæœç´¢")
    print("="*60)

    db_config = get_db_config()
    service = get_semantic_service()

    async with db_config.session() as session:
        # 1. å‰µå»ºæ¸¬è©¦æ–‡ç« 
        print("\n1ï¸âƒ£ å‰µå»ºæ¸¬è©¦æ–‡ç« ...")

        test_articles = [
            {
                "title": "æ·±åº¦å­¸ç¿’åœ¨è¨ˆç®—æ©Ÿè¦–è¦ºä¸­çš„æ‡‰ç”¨",
                "body": "æ·±åº¦å­¸ç¿’æŠ€è¡“ï¼Œç‰¹åˆ¥æ˜¯å·ç©ç¥ç¶“ç¶²çµ¡ï¼ˆCNNï¼‰ï¼Œå·²ç¶“å¾¹åº•æ”¹è®Šäº†è¨ˆç®—æ©Ÿè¦–è¦ºé ˜åŸŸã€‚å¾åœ–åƒåˆ†é¡åˆ°ç›®æ¨™æª¢æ¸¬ï¼Œå¾èªç¾©åˆ†å‰²åˆ°äººè‡‰è­˜åˆ¥ï¼Œæ·±åº¦å­¸ç¿’æ¨¡å‹åœ¨å„ç¨®è¦–è¦ºä»»å‹™ä¸­éƒ½å–å¾—äº†çªç ´æ€§é€²å±•ã€‚æœ¬æ–‡å°‡æ¢è¨æ·±åº¦å­¸ç¿’åœ¨è¨ˆç®—æ©Ÿè¦–è¦ºä¸­çš„ä¸»è¦æ‡‰ç”¨ï¼ŒåŒ…æ‹¬é†«ç™‚å½±åƒåˆ†æã€è‡ªå‹•é§•é§›ã€å®‰é˜²ç›£æ§ç­‰é ˜åŸŸçš„å¯¦éš›æ¡ˆä¾‹ã€‚"
            },
            {
                "title": "è‡ªç„¶èªè¨€è™•ç†çš„æœ€æ–°é€²å±•",
                "body": "è¿‘å¹´ä¾†ï¼Œè‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰é ˜åŸŸç¶“æ­·äº†é©å‘½æ€§çš„è®ŠåŒ–ã€‚Transformer æ¶æ§‹çš„å‡ºç¾ï¼Œä»¥åŠ BERTã€GPT ç­‰é è¨“ç·´æ¨¡å‹çš„æˆåŠŸï¼Œä½¿å¾—æ©Ÿå™¨ç†è§£å’Œç”Ÿæˆäººé¡èªè¨€çš„èƒ½åŠ›å¤§å¹…æå‡ã€‚æœ¬æ–‡å°‡ä»‹ç´¹ NLP çš„æœ€æ–°æŠ€è¡“é€²å±•ï¼ŒåŒ…æ‹¬å¤§èªè¨€æ¨¡å‹ã€é›¶æ¨£æœ¬å­¸ç¿’ã€å¤šèªè¨€ç†è§£ç­‰æ–¹é¢çš„çªç ´ã€‚"
            },
            {
                "title": "é‡å­è¨ˆç®—çš„æœªä¾†å±•æœ›",
                "body": "é‡å­è¨ˆç®—ä½œç‚ºä¸‹ä¸€ä»£è¨ˆç®—æŠ€è¡“ï¼Œæœ‰æœ›åœ¨æŸäº›ç‰¹å®šå•é¡Œä¸Šå¯¦ç¾æŒ‡æ•¸ç´šçš„åŠ é€Ÿã€‚èˆ‡å‚³çµ±è¨ˆç®—æ©Ÿä½¿ç”¨æ¯”ç‰¹ä¸åŒï¼Œé‡å­è¨ˆç®—æ©Ÿä½¿ç”¨é‡å­æ¯”ç‰¹ï¼ˆqubitï¼‰ï¼Œå¯ä»¥åŒæ™‚è™•æ–¼å¤šå€‹ç‹€æ…‹çš„ç–ŠåŠ ã€‚æœ¬æ–‡æ¢è¨é‡å­è¨ˆç®—çš„åŸºæœ¬åŸç†ã€ç•¶å‰æŒ‘æˆ°ä»¥åŠåœ¨å¯†ç¢¼å­¸ã€è—¥ç‰©ç ”ç™¼ã€é‡‘èå»ºæ¨¡ç­‰é ˜åŸŸçš„æ½›åœ¨æ‡‰ç”¨ã€‚"
            },
            {
                "title": "è¨ˆç®—æ©Ÿè¦–è¦ºèˆ‡æ·±åº¦å­¸ç¿’çš„èåˆ",
                "body": "è¨ˆç®—æ©Ÿè¦–è¦ºå’Œæ·±åº¦å­¸ç¿’çš„çµåˆå‰µé€ äº†è¨±å¤šä»¤äººé©šå˜†çš„æ‡‰ç”¨ã€‚æ·±åº¦å·ç©ç¥ç¶“ç¶²çµ¡èƒ½å¤ è‡ªå‹•å­¸ç¿’åœ–åƒç‰¹å¾µï¼Œç„¡éœ€äººå·¥è¨­è¨ˆã€‚é€™ç¨®æ–¹æ³•åœ¨åœ–åƒè­˜åˆ¥ã€ç‰©é«”æª¢æ¸¬ã€åœ–åƒç”Ÿæˆç­‰ä»»å‹™ä¸Šéƒ½å–å¾—äº†è¶…è¶Šäººé¡çš„æ€§èƒ½ã€‚æœ¬æ–‡æ·±å…¥æ¢è¨é€™ç¨®èåˆå¸¶ä¾†çš„æŠ€è¡“é©æ–°ã€‚"
            }
        ]

        created_articles = []
        for article_data in test_articles:
            # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
            existing = await session.execute(
                select(Article).where(Article.title == article_data["title"])
            )
            article = existing.scalar_one_or_none()

            if not article:
                article = Article(**article_data, status="published")
                session.add(article)
                await session.commit()
                print(f"âœ… å‰µå»ºæ–‡ç« : {article.title}")
            else:
                print(f"â„¹ï¸  æ–‡ç« å·²å­˜åœ¨: {article.title}")

            created_articles.append(article)

        # 2. ç”Ÿæˆä¸¦å­˜å„²åµŒå…¥
        print("\n2ï¸âƒ£ ç”Ÿæˆä¸¦å­˜å„²æ–‡ç« åµŒå…¥...")

        for article in created_articles:
            try:
                await service.store_article_embedding(session, article.id)
                print(f"âœ… å­˜å„²åµŒå…¥: æ–‡ç«  {article.id} - {article.title[:30]}...")
            except Exception as e:
                print(f"âŒ å¤±æ•—: {e}")

        # 3. æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« 
        print("\n3ï¸âƒ£ æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« ...")

        query = "æ·±åº¦å­¸ç¿’å’Œè¨ˆç®—æ©Ÿè¦–è¦ºçš„æœ€æ–°æ‡‰ç”¨"
        print(f"\næŸ¥è©¢: {query}")

        similar = await service.find_similar_articles(
            session,
            query,
            limit=5,
            threshold=0.7
        )

        if similar:
            print(f"\næ‰¾åˆ° {len(similar)} ç¯‡ç›¸ä¼¼æ–‡ç« :")
            for i, (article, similarity) in enumerate(similar, 1):
                print(f"{i}. {article.title}")
                print(f"   ç›¸ä¼¼åº¦: {similarity:.2%}")
                print(f"   æ‘˜è¦: {article.body[:100]}...")
                print()
        else:
            print("æœªæ‰¾åˆ°ç›¸ä¼¼æ–‡ç« ")

        # 4. æª¢æ¸¬é‡è¤‡å…§å®¹
        print("\n4ï¸âƒ£ æª¢æ¸¬é‡è¤‡å…§å®¹...")

        # æ¸¬è©¦å¹¾ä¹ç›¸åŒçš„å…§å®¹
        duplicate_text = "æ·±åº¦å­¸ç¿’æŠ€è¡“ï¼Œå°¤å…¶æ˜¯å·ç©ç¥ç¶“ç¶²çµ¡ï¼ˆCNNï¼‰ï¼Œå·²ç¶“å¾¹åº•æ”¹è®Šäº†è¨ˆç®—æ©Ÿè¦–è¦ºé ˜åŸŸã€‚å¾åœ–åƒåˆ†é¡åˆ°ç›®æ¨™æª¢æ¸¬ï¼Œæ·±åº¦å­¸ç¿’åœ¨å„ç¨®è¦–è¦ºä»»å‹™ä¸­éƒ½å–å¾—äº†çªç ´ã€‚"

        print(f"\næª¢æŸ¥æ–‡æœ¬: {duplicate_text[:50]}...")

        duplicate = await service.check_duplicate_content(
            session,
            duplicate_text,
            threshold=0.90
        )

        if duplicate:
            print("âš ï¸  ç™¼ç¾é‡è¤‡å…§å®¹!")
            print(f"   æ–‡ç« : {duplicate.title}")
            print(f"   ID: {duplicate.id}")
        else:
            print("âœ… æœªç™¼ç¾é‡è¤‡å…§å®¹")

        return created_articles


async def test_batch_operations():
    """æ¸¬è©¦æ‰¹é‡æ“ä½œ"""
    print("\n" + "="*60)
    print("âš¡ æ¸¬è©¦ 3: æ‰¹é‡åµŒå…¥ç”Ÿæˆ")
    print("="*60)

    service = get_semantic_service()

    # æº–å‚™æ‰¹é‡æ–‡æœ¬
    batch_texts = [
        f"é€™æ˜¯ç¬¬ {i} ç¯‡é—œæ–¼ {topic} çš„æ–‡ç« å…§å®¹"
        for i in range(1, 11)
        for topic in ["AI", "æ©Ÿå™¨å­¸ç¿’", "æ•¸æ“šç§‘å­¸"]
    ]

    print(f"\nç”Ÿæˆ {len(batch_texts)} å€‹æ–‡æœ¬çš„åµŒå…¥...")

    start_time = datetime.now()
    embeddings = await service.batch_generate_embeddings(batch_texts, batch_size=10)
    end_time = datetime.now()

    success_count = sum(1 for e in embeddings if e is not None)

    print(f"âœ… å®Œæˆï¼æˆåŠŸ: {success_count}/{len(batch_texts)}")
    print(f"â±ï¸  è€—æ™‚: {(end_time - start_time).total_seconds():.2f} ç§’")


async def test_similarity_search():
    """æ¸¬è©¦ç›¸ä¼¼åº¦æœç´¢çš„é«˜ç´šåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ” æ¸¬è©¦ 4: é«˜ç´šç›¸ä¼¼åº¦æœç´¢")
    print("="*60)

    db_config = get_db_config()
    service = get_semantic_service()

    async with db_config.session() as session:
        # 1. æ¸¬è©¦æ’é™¤ç‰¹å®šæ–‡ç« 
        print("\n1ï¸âƒ£ æœç´¢æ™‚æ’é™¤ç‰¹å®šæ–‡ç« ...")

        # ç²å–ä¸€ç¯‡æ–‡ç« ä½œç‚ºæŸ¥è©¢
        result = await session.execute(
            select(Article).limit(1)
        )
        sample_article = result.scalar_one_or_none()

        if sample_article:
            query_text = f"{sample_article.title} {sample_article.body[:200]}"

            # æœç´¢ä½†æ’é™¤è‡ªå·±
            similar = await service.find_similar_articles(
                session,
                query_text,
                limit=3,
                threshold=0.5,
                exclude_ids=[sample_article.id]
            )

            print(f"æŸ¥è©¢æ–‡ç« : {sample_article.title}")
            print(f"æ’é™¤ ID: {sample_article.id}")

            if similar:
                print(f"\næ‰¾åˆ° {len(similar)} ç¯‡ç›¸ä¼¼æ–‡ç« ï¼ˆæ’é™¤è‡ªå·±ï¼‰:")
                for article, similarity in similar:
                    print(f"  - {article.title[:50]}... (ç›¸ä¼¼åº¦: {similarity:.2%})")
            else:
                print("æœªæ‰¾åˆ°å…¶ä»–ç›¸ä¼¼æ–‡ç« ")

        # 2. æ¸¬è©¦ä¸åŒçš„ç›¸ä¼¼åº¦é–¾å€¼
        print("\n2ï¸âƒ£ æ¸¬è©¦ä¸åŒç›¸ä¼¼åº¦é–¾å€¼...")

        query = "äººå·¥æ™ºèƒ½å’Œæ©Ÿå™¨å­¸ç¿’"
        thresholds = [0.9, 0.8, 0.7, 0.6, 0.5]

        for threshold in thresholds:
            similar = await service.find_similar_articles(
                session,
                query,
                limit=100,
                threshold=threshold
            )
            print(f"  é–¾å€¼ {threshold}: æ‰¾åˆ° {len(similar)} ç¯‡æ–‡ç« ")


async def test_performance():
    """æ¸¬è©¦æ€§èƒ½"""
    print("\n" + "="*60)
    print("âš¡ æ¸¬è©¦ 5: æ€§èƒ½æ¸¬è©¦")
    print("="*60)

    service = get_semantic_service()

    # 1. æ¸¬è©¦ç·©å­˜æ•ˆæœ
    print("\n1ï¸âƒ£ æ¸¬è©¦ç·©å­˜æ•ˆæœ...")

    test_text = "é€™æ˜¯ä¸€æ®µç”¨æ–¼æ¸¬è©¦ç·©å­˜çš„æ–‡æœ¬å…§å®¹"

    # ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ˆç„¡ç·©å­˜ï¼‰
    start = datetime.now()
    embedding1 = await service.generate_embedding(test_text)
    time1 = (datetime.now() - start).total_seconds()

    # ç¬¬äºŒæ¬¡ç”Ÿæˆï¼ˆæœ‰ç·©å­˜ï¼‰
    start = datetime.now()
    embedding2 = await service.generate_embedding(test_text)
    time2 = (datetime.now() - start).total_seconds()

    print(f"ç¬¬ä¸€æ¬¡ç”Ÿæˆ: {time1:.3f} ç§’")
    print(f"ç¬¬äºŒæ¬¡ç”Ÿæˆ: {time2:.3f} ç§’ï¼ˆç·©å­˜ï¼‰")
    print(f"åŠ é€Ÿæ¯”: {time1/time2:.1f}x" if time2 > 0 else "ç„¡é™å¤§")

    # é©—è­‰çµæœä¸€è‡´
    import numpy as np
    if np.array_equal(embedding1, embedding2):
        print("âœ… ç·©å­˜çµæœä¸€è‡´")
    else:
        print("âŒ ç·©å­˜çµæœä¸ä¸€è‡´")

    # 2. æ¸…é™¤ç·©å­˜
    print("\n2ï¸âƒ£ æ¸…é™¤ç·©å­˜...")
    await service.clear_cache()
    print("âœ… ç·©å­˜å·²æ¸…é™¤")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("="*60)
    print("ğŸ§ª èªç¾©ç›¸ä¼¼åº¦æª¢æ¸¬åŠŸèƒ½æ¸¬è©¦")
    print("="*60)

    try:
        # é‹è¡Œå„é …æ¸¬è©¦
        embedding = await test_basic_embedding()

        if embedding:
            await test_article_operations()
            await test_batch_operations()
            await test_similarity_search()
            await test_performance()
        else:
            print("\nâš ï¸  åµŒå…¥ç”Ÿæˆå¤±æ•—ï¼Œè·³éå¾ŒçºŒæ¸¬è©¦")
            print("è«‹æª¢æŸ¥ OpenAI API å¯†é‘°é…ç½®")

        print("\n" + "="*60)
        print("âœ… æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")
        print("="*60)

    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(main())
