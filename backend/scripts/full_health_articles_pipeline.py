#!/usr/bin/env python3
"""
å¤§ç´€å…ƒå¥åº·æ–‡ç« å…¨è‡ªå‹•è™•ç† Pipeline

é€™å€‹è…³æœ¬æœƒè‡ªå‹•å®Œæˆï¼š
1. æŠ“å–æ‰€æœ‰å¥åº·é¡æ–‡ç« ï¼ˆç´„ 15,000+ ç¯‡ï¼‰
2. AI è§£æï¼ˆé—œéµè©ã€æ¨™é¡Œåˆ†è§£ã€åˆ†é¡ï¼‰
3. å‘é‡åŒ–ï¼ˆç”ŸæˆåµŒå…¥å‘é‡ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/full_health_articles_pipeline.py

å¯é¸åƒæ•¸ï¼š
    --batch-size: æ¯æ‰¹æŠ“å–æ–‡ç« æ•¸ï¼ˆé»˜èª 100ï¼‰
    --parse-batch: AI è§£ææ‰¹æ¬¡å¤§å°ï¼ˆé»˜èª 20ï¼‰
    --embed-batch: å‘é‡åŒ–æ‰¹æ¬¡å¤§å°ï¼ˆé»˜èª 50ï¼‰
    --start-page: èµ·å§‹é ç¢¼ï¼ˆé»˜èª 1ï¼‰
    --max-articles: æœ€å¤§æ–‡ç« æ•¸ï¼ˆé»˜èª 0 è¡¨ç¤ºç„¡é™åˆ¶ï¼‰
    --category: åˆ†é¡ç´¢å¼•ï¼ˆ0=å¥åº·é¤Šç”Ÿ, 1=é£Ÿç™‚é¤Šç”Ÿ, 2=å¥åº·ç”Ÿæ´», é»˜èªè™•ç†å…¨éƒ¨ï¼‰
    --dry-run: è©¦é‹è¡Œæ¨¡å¼ï¼Œä¸å¯¦éš›å„²å­˜
"""

import os
import sys
import time
import argparse
import json
import logging
from datetime import datetime
from dataclasses import dataclass, field
from typing import Optional, List, Dict, Any

import httpx
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============== é…ç½® ==============

BASE_URL = "https://www.epochtimes.com"
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

# å¥åº·åˆ†é¡
HEALTH_CATEGORIES = [
    {"url": "/b5/nf2283.htm", "name": "å¥åº·é¤Šç”Ÿ", "max_pages": 410},
    {"url": "/b5/ncid248.htm", "name": "é£Ÿç™‚é¤Šç”Ÿ", "max_pages": 15},
    {"url": "/b5/ncid246.htm", "name": "å¥åº·ç”Ÿæ´»", "max_pages": 15},
]

# å»¶é²è¨­å®šï¼ˆç§’ï¼‰
LIST_PAGE_DELAY = 1.0
DETAIL_PAGE_DELAY = 1.5
PARSE_DELAY = 0.5
EMBED_DELAY = 0.3

# é‡è©¦è¨­å®š
MAX_RETRIES = 3
RETRY_DELAY = 10  # ç§’

# ============== è³‡æ–™çµæ§‹ ==============

@dataclass
class PipelineStats:
    """Pipeline çµ±è¨ˆ"""
    start_time: datetime = field(default_factory=datetime.now)

    # æŠ“å–çµ±è¨ˆ
    pages_scanned: int = 0
    urls_collected: int = 0
    articles_scraped: int = 0
    articles_skipped: int = 0
    scrape_errors: int = 0

    # è§£æçµ±è¨ˆ
    articles_parsed: int = 0
    parse_errors: int = 0
    tokens_used: int = 0

    # å‘é‡åŒ–çµ±è¨ˆ
    articles_embedded: int = 0
    embed_errors: int = 0

    def elapsed_time(self) -> str:
        """è¿”å›ç¶“éæ™‚é–“"""
        elapsed = datetime.now() - self.start_time
        hours, remainder = divmod(int(elapsed.total_seconds()), 3600)
        minutes, seconds = divmod(remainder, 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

    def summary(self) -> str:
        """è¿”å›çµ±è¨ˆæ‘˜è¦"""
        return f"""
============================================================
Pipeline åŸ·è¡Œå®Œæˆ
============================================================
åŸ·è¡Œæ™‚é–“: {self.elapsed_time()}

ğŸ“¥ æŠ“å–éšæ®µ:
   - æƒæé æ•¸: {self.pages_scanned}
   - æ”¶é›† URL: {self.urls_collected}
   - æ–°å¢æ–‡ç« : {self.articles_scraped}
   - è·³éæ–‡ç« : {self.articles_skipped}
   - éŒ¯èª¤æ•¸: {self.scrape_errors}

ğŸ”„ è§£æéšæ®µ:
   - å·²è§£æ: {self.articles_parsed}
   - Token ä½¿ç”¨: {self.tokens_used:,}
   - éŒ¯èª¤æ•¸: {self.parse_errors}

âœ… å‘é‡åŒ–éšæ®µ:
   - å·²å‘é‡åŒ–: {self.articles_embedded}
   - éŒ¯èª¤æ•¸: {self.embed_errors}
============================================================
"""


@dataclass
class ArticleData:
    """æ–‡ç« è³‡æ–™"""
    article_id: str
    url: str
    title: str
    author: str
    publish_date: str
    content: str
    word_count: int
    images: List[str]
    category: str
    source_category: str


# ============== HTTP å®¢æˆ¶ç«¯ ==============

def get_http_client() -> httpx.Client:
    """ç²å– HTTP å®¢æˆ¶ç«¯"""
    return httpx.Client(
        headers={
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.8",
        },
        timeout=30.0,
        follow_redirects=True,
    )


def get_supabase_client():
    """ç²å– Supabase å®¢æˆ¶ç«¯"""
    from supabase import create_client
    return create_client(SUPABASE_URL, SUPABASE_KEY)


# ============== æŠ“å–å‡½æ•¸ ==============

def scrape_list_page(client: httpx.Client, category_url: str, page: int) -> List[Dict]:
    """æŠ“å–åˆ—è¡¨é """
    if page == 1:
        url = f"{BASE_URL}{category_url}"
    else:
        base = category_url.rsplit(".", 1)[0]
        url = f"{BASE_URL}{base}_{page}.htm"

    try:
        response = client.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        articles = []

        # è§£ææ–‡ç« é€£çµ
        for link in soup.select("a[href*='/b5/'][href$='.htm']"):
            href = link.get("href", "")
            if "/b5/" in href and href.endswith(".htm"):
                # æå–æ–‡ç«  ID
                import re
                match = re.search(r"/(n\d+)\.htm", href)
                if match:
                    article_id = match.group(1)
                    full_url = href if href.startswith("http") else f"{BASE_URL}{href}"
                    articles.append({
                        "article_id": article_id,
                        "url": full_url,
                    })

        # å»é‡
        seen = set()
        unique = []
        for a in articles:
            if a["article_id"] not in seen:
                seen.add(a["article_id"])
                unique.append(a)

        return unique

    except Exception as e:
        logger.error(f"æŠ“å–åˆ—è¡¨é å¤±æ•—: {url} - {e}")
        return []


def scrape_article_detail(client: httpx.Client, url: str, category: str) -> Optional[ArticleData]:
    """æŠ“å–æ–‡ç« è©³æƒ…"""
    try:
        response = client.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # æå–æ¨™é¡Œ
        title_elem = soup.select_one("h1.title") or soup.select_one("h1")
        title = title_elem.get_text(strip=True) if title_elem else ""

        # æå–ä½œè€…
        author_elem = soup.select_one(".author") or soup.select_one("[class*='author']")
        author = author_elem.get_text(strip=True) if author_elem else ""

        # æå–æ—¥æœŸ
        date_elem = soup.select_one(".post_date") or soup.select_one("time") or soup.select_one("[class*='date']")
        publish_date = date_elem.get_text(strip=True) if date_elem else ""

        # æå–å…§å®¹
        content_elem = soup.select_one("div.post_content") or soup.select_one("article") or soup.select_one(".content")
        content = ""
        if content_elem:
            # ç§»é™¤è…³æœ¬å’Œæ¨£å¼
            for tag in content_elem.select("script, style, nav, footer"):
                tag.decompose()
            content = content_elem.get_text(separator="\n", strip=True)

        # æå–åœ–ç‰‡
        images = []
        if content_elem:
            for img in content_elem.select("img[src]"):
                src = img.get("src", "")
                if src and not src.startswith("data:"):
                    images.append(src)

        # æå–æ–‡ç«  ID
        import re
        match = re.search(r"/(n\d+)\.htm", url)
        article_id = match.group(1) if match else ""

        if not title or not article_id:
            return None

        return ArticleData(
            article_id=article_id,
            url=url,
            title=title,
            author=author,
            publish_date=publish_date,
            content=content,
            word_count=len(content),
            images=images,
            category=category,
            source_category=category,
        )

    except Exception as e:
        logger.error(f"æŠ“å–æ–‡ç« è©³æƒ…å¤±æ•—: {url} - {e}")
        return None


def check_article_exists(supabase, article_id: str) -> bool:
    """æª¢æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
    try:
        result = supabase.table("health_articles").select("article_id").eq("article_id", article_id).limit(1).execute()
        return len(result.data) > 0
    except Exception:
        return False


def parse_publish_date(date_str: str) -> Optional[str]:
    """è§£æç™¼å¸ƒæ—¥æœŸï¼Œè¿”å› YYYY-MM-DD æ ¼å¼æˆ– None"""
    if not date_str:
        return None

    import re

    # å˜—è©¦æå–æ—¥æœŸæ¨¡å¼
    patterns = [
        # 2015-10-22
        r"(\d{4}-\d{2}-\d{2})",
        # 2015/10/22
        r"(\d{4}/\d{2}/\d{2})",
        # 2015å¹´10æœˆ22æ—¥
        r"(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",
    ]

    for pattern in patterns:
        match = re.search(pattern, date_str)
        if match:
            if len(match.groups()) == 1:
                # ç›´æ¥æ˜¯æ—¥æœŸæ ¼å¼
                return match.group(1).replace("/", "-")
            elif len(match.groups()) == 3:
                # å¹´æœˆæ—¥æ ¼å¼
                year, month, day = match.groups()
                return f"{year}-{int(month):02d}-{int(day):02d}"

    return None


def save_article(supabase, article: ArticleData) -> bool:
    """å„²å­˜æ–‡ç« åˆ°è³‡æ–™åº«"""
    try:
        # è§£ææ—¥æœŸ
        parsed_date = parse_publish_date(article.publish_date)

        # ä½¿ç”¨èˆ‡ health_articles è¡¨åŒ¹é…çš„æ¬„ä½åç¨±
        data = {
            "article_id": article.article_id,
            "original_url": article.url,
            "title": article.title,
            "author_line": article.author,  # åŸå§‹ä½œè€…è¡Œ
            "body_html": article.content,  # ä½¿ç”¨ body_html æ¬„ä½
            "word_count": article.word_count,
            "category": article.category,
            "primary_category": article.source_category,
            "status": "scraped",
            "scraped_at": datetime.now().isoformat(),
        }

        # åªæœ‰æœ‰æ•ˆæ—¥æœŸæ‰åŠ å…¥
        if parsed_date:
            data["publish_date"] = parsed_date

        supabase.table("health_articles").upsert(data, on_conflict="article_id").execute()
        return True

    except Exception as e:
        logger.error(f"å„²å­˜æ–‡ç« å¤±æ•—: {article.article_id} - {e}")
        return False


# ============== è§£æå‡½æ•¸ ==============

def call_parse_api(batch_size: int = 20) -> Dict[str, Any]:
    """èª¿ç”¨ parse-articles Edge Functionï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰"""
    for attempt in range(MAX_RETRIES):
        try:
            with httpx.Client(timeout=180.0) as client:  # å¢åŠ è¶…æ™‚æ™‚é–“
                response = client.post(
                    f"{SUPABASE_URL}/functions/v1/parse-articles",
                    headers={
                        "Authorization": f"Bearer {SUPABASE_KEY}",
                        "Content-Type": "application/json",
                    },
                    json={"batchSize": batch_size},
                )
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logger.warning(f"èª¿ç”¨ parse-articles å¤±æ•— (å˜—è©¦ {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                logger.info(f"ç­‰å¾… {RETRY_DELAY} ç§’å¾Œé‡è©¦...")
                time.sleep(RETRY_DELAY)
            else:
                logger.error(f"parse-articles é‡è©¦ {MAX_RETRIES} æ¬¡å¾Œä»å¤±æ•—")
                return {"processed": 0, "failed": 1, "error": str(e), "retry_exhausted": True}


def call_embed_api(batch_size: int = 50) -> Dict[str, Any]:
    """èª¿ç”¨ generate-embeddings Edge Functionï¼ˆå«é‡è©¦æ©Ÿåˆ¶ï¼‰"""
    for attempt in range(MAX_RETRIES):
        try:
            with httpx.Client(timeout=180.0) as client:  # å¢åŠ è¶…æ™‚æ™‚é–“
                response = client.post(
                    f"{SUPABASE_URL}/functions/v1/generate-embeddings",
                    headers={
                        "Authorization": f"Bearer {SUPABASE_KEY}",
                        "Content-Type": "application/json",
                    },
                    json={"batchSize": batch_size},
                )
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logger.warning(f"èª¿ç”¨ generate-embeddings å¤±æ•— (å˜—è©¦ {attempt + 1}/{MAX_RETRIES}): {e}")
            if attempt < MAX_RETRIES - 1:
                logger.info(f"ç­‰å¾… {RETRY_DELAY} ç§’å¾Œé‡è©¦...")
                time.sleep(RETRY_DELAY)
            else:
                logger.error(f"generate-embeddings é‡è©¦ {MAX_RETRIES} æ¬¡å¾Œä»å¤±æ•—")
                return {"processed": 0, "failed": 1, "error": str(e), "retry_exhausted": True}


# ============== ä¸»è¦ Pipeline ==============

def run_scrape_phase(
    stats: PipelineStats,
    http_client: httpx.Client,
    supabase,
    categories: List[Dict],
    start_page: int,
    batch_size: int,
    max_articles: int,
    dry_run: bool,
) -> None:
    """Phase 1: æŠ“å–æ–‡ç«  - æŒçºŒè™•ç†æ‰€æœ‰é é¢"""
    logger.info("=" * 60)
    logger.info("Phase 1: æŠ“å–æ–‡ç« ï¼ˆæŒçºŒæ¨¡å¼ï¼‰")
    logger.info("=" * 60)

    total_new = 0

    for cat_idx, category in enumerate(categories):
        if max_articles > 0 and total_new >= max_articles:
            break

        cat_name = category["name"]
        cat_url = category["url"]
        cat_max_pages = category.get("max_pages", 500)

        logger.info(f"\nğŸ“‚ è™•ç†åˆ†é¡ [{cat_idx+1}/{len(categories)}]: {cat_name}")
        logger.info(f"   ç¸½é æ•¸: {cat_max_pages}, èµ·å§‹é : {start_page}")

        # æŒçºŒè™•ç†æ‰€æœ‰é é¢
        consecutive_empty_pages = 0  # é€£çºŒç„¡æ–°æ–‡ç« çš„é æ•¸
        max_consecutive_empty = 10   # é€£çºŒ 10 é ç„¡æ–°æ–‡ç« æ™‚è·³éæ­¤åˆ†é¡

        for page in range(start_page, start_page + cat_max_pages):
            if max_articles > 0 and total_new >= max_articles:
                logger.info(f"  å·²é”æœ€å¤§æ–‡ç« æ•¸é™åˆ¶ ({max_articles})")
                break

            # ç²å–è©²é çš„æ–‡ç« åˆ—è¡¨
            articles = scrape_list_page(http_client, cat_url, page)
            stats.pages_scanned += 1

            if not articles:
                logger.warning(f"  é  {page} ç„¡æ–‡ç« ï¼Œåœæ­¢æ­¤åˆ†é¡")
                break

            stats.urls_collected += len(articles)

            # è™•ç†è©²é çš„æ‰€æœ‰æ–‡ç« 
            page_new = 0
            page_skipped = 0

            for article_info in articles:
                if max_articles > 0 and total_new >= max_articles:
                    break

                article_id = article_info["article_id"]

                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if not dry_run and check_article_exists(supabase, article_id):
                    stats.articles_skipped += 1
                    page_skipped += 1
                    continue

                # æŠ“å–è©³æƒ…
                article = scrape_article_detail(http_client, article_info["url"], cat_name)

                if not article:
                    stats.scrape_errors += 1
                    continue

                # å„²å­˜
                if not dry_run:
                    if save_article(supabase, article):
                        stats.articles_scraped += 1
                        total_new += 1
                        page_new += 1
                    else:
                        stats.scrape_errors += 1
                else:
                    stats.articles_scraped += 1
                    total_new += 1
                    page_new += 1

                time.sleep(DETAIL_PAGE_DELAY)

            # æ¯é å ±å‘Šé€²åº¦
            logger.info(f"  é  {page}: æ–°å¢ {page_new}, è·³é {page_skipped}, ç¸½è¨ˆæ–°å¢ {total_new}")

            # æª¢æŸ¥æ˜¯å¦é€£çºŒç„¡æ–°æ–‡ç« 
            if page_new == 0:
                consecutive_empty_pages += 1
                if consecutive_empty_pages >= max_consecutive_empty:
                    logger.info(f"  é€£çºŒ {max_consecutive_empty} é ç„¡æ–°æ–‡ç« ï¼Œè·³éæ­¤åˆ†é¡å‰©é¤˜é é¢")
                    break
            else:
                consecutive_empty_pages = 0

            time.sleep(LIST_PAGE_DELAY)

        logger.info(f"  åˆ†é¡ {cat_name} å®Œæˆ: ç¸½å…±æ–°å¢ {total_new} ç¯‡")


def run_parse_phase(stats: PipelineStats, parse_batch: int, dry_run: bool) -> None:
    """Phase 2: AI è§£æ"""
    logger.info("\n" + "=" * 60)
    logger.info("Phase 2: AI è§£æ")
    logger.info("=" * 60)

    if dry_run:
        logger.info("è©¦é‹è¡Œæ¨¡å¼ï¼Œè·³éè§£æ")
        return

    batch_num = 0
    consecutive_failures = 0
    max_consecutive_failures = 5  # é€£çºŒå¤±æ•—è¶…éæ­¤æ•¸æ‰åœæ­¢

    while True:
        batch_num += 1
        result = call_parse_api(parse_batch)

        processed = result.get("processed", 0)
        failed = result.get("failed", 0)
        tokens = result.get("tokensUsed", 0)
        retry_exhausted = result.get("retry_exhausted", False)

        stats.articles_parsed += processed
        stats.parse_errors += failed
        stats.tokens_used += tokens

        if processed > 0:
            logger.info(f"  æ‰¹æ¬¡ {batch_num}: è§£æ {processed} ç¯‡, tokens: {tokens}")
            consecutive_failures = 0  # é‡ç½®é€£çºŒå¤±æ•—è¨ˆæ•¸

        # å¦‚æœé‡è©¦è€—ç›¡ä½†ä»æœ‰å¾…è™•ç†æ–‡ç« ï¼Œç­‰å¾…å¾Œé‡è©¦
        if retry_exhausted:
            consecutive_failures += 1
            logger.warning(f"  é€£çºŒå¤±æ•— {consecutive_failures}/{max_consecutive_failures}")
            if consecutive_failures >= max_consecutive_failures:
                logger.error("  é€£çºŒå¤±æ•—éå¤šï¼Œåœæ­¢è§£æéšæ®µ")
                break
            logger.info(f"  ç­‰å¾… 30 ç§’å¾Œé‡è©¦...")
            time.sleep(30)
            continue

        if processed == 0 and not retry_exhausted:
            break

        time.sleep(PARSE_DELAY)

    logger.info(f"  è§£æå®Œæˆ: å…± {stats.articles_parsed} ç¯‡")


def run_embed_phase(stats: PipelineStats, embed_batch: int, dry_run: bool) -> None:
    """Phase 3: å‘é‡åŒ–"""
    logger.info("\n" + "=" * 60)
    logger.info("Phase 3: å‘é‡åŒ–")
    logger.info("=" * 60)

    if dry_run:
        logger.info("è©¦é‹è¡Œæ¨¡å¼ï¼Œè·³éå‘é‡åŒ–")
        return

    batch_num = 0
    consecutive_failures = 0
    max_consecutive_failures = 5  # é€£çºŒå¤±æ•—è¶…éæ­¤æ•¸æ‰åœæ­¢

    while True:
        batch_num += 1
        result = call_embed_api(embed_batch)

        processed = result.get("processed", 0)
        failed = result.get("failed", 0)
        retry_exhausted = result.get("retry_exhausted", False)

        stats.articles_embedded += processed
        stats.embed_errors += failed

        if processed > 0:
            logger.info(f"  æ‰¹æ¬¡ {batch_num}: å‘é‡åŒ– {processed} ç¯‡")
            consecutive_failures = 0  # é‡ç½®é€£çºŒå¤±æ•—è¨ˆæ•¸

        # å¦‚æœé‡è©¦è€—ç›¡ä½†ä»æœ‰å¾…è™•ç†æ–‡ç« ï¼Œç­‰å¾…å¾Œé‡è©¦
        if retry_exhausted:
            consecutive_failures += 1
            logger.warning(f"  é€£çºŒå¤±æ•— {consecutive_failures}/{max_consecutive_failures}")
            if consecutive_failures >= max_consecutive_failures:
                logger.error("  é€£çºŒå¤±æ•—éå¤šï¼Œåœæ­¢å‘é‡åŒ–éšæ®µ")
                break
            logger.info(f"  ç­‰å¾… 30 ç§’å¾Œé‡è©¦...")
            time.sleep(30)
            continue

        if processed == 0 and not retry_exhausted:
            break

        time.sleep(EMBED_DELAY)

    logger.info(f"  å‘é‡åŒ–å®Œæˆ: å…± {stats.articles_embedded} ç¯‡")


def run_full_pipeline(
    batch_size: int = 100,
    parse_batch: int = 20,
    embed_batch: int = 50,
    start_page: int = 1,
    max_articles: int = 0,
    category_index: Optional[int] = None,
    dry_run: bool = False,
) -> PipelineStats:
    """åŸ·è¡Œå®Œæ•´ Pipeline"""

    stats = PipelineStats()

    logger.info("=" * 60)
    logger.info("å¤§ç´€å…ƒå¥åº·æ–‡ç« å…¨è‡ªå‹•è™•ç† Pipeline")
    logger.info("=" * 60)
    logger.info(f"æ¨¡å¼: {'è©¦é‹è¡Œ' if dry_run else 'æ­£å¼é‹è¡Œ'}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: æŠ“å–={batch_size}, è§£æ={parse_batch}, å‘é‡åŒ–={embed_batch}")
    logger.info(f"èµ·å§‹é : {start_page}")
    logger.info(f"æœ€å¤§æ–‡ç« æ•¸: {max_articles if max_articles > 0 else 'ç„¡é™åˆ¶'}")

    # ç¢ºå®šè¦è™•ç†çš„åˆ†é¡
    if category_index is not None:
        categories = [HEALTH_CATEGORIES[category_index]]
        logger.info(f"åˆ†é¡: {categories[0]['name']}")
    else:
        categories = HEALTH_CATEGORIES
        logger.info(f"åˆ†é¡: å…¨éƒ¨ ({len(categories)} å€‹)")

    # åˆå§‹åŒ–å®¢æˆ¶ç«¯
    http_client = get_http_client()
    supabase = None if dry_run else get_supabase_client()

    if not dry_run:
        # é¡¯ç¤ºåˆå§‹ç‹€æ…‹
        result = supabase.table("health_articles").select("count", count="exact").execute()
        logger.info(f"è³‡æ–™åº«ç¾æœ‰æ–‡ç« : {result.count}")

    try:
        # Phase 1: æŠ“å–
        run_scrape_phase(
            stats=stats,
            http_client=http_client,
            supabase=supabase,
            categories=categories,
            start_page=start_page,
            batch_size=batch_size,
            max_articles=max_articles,
            dry_run=dry_run,
        )

        # Phase 2: è§£æ
        run_parse_phase(stats, parse_batch, dry_run)

        # Phase 3: å‘é‡åŒ–
        run_embed_phase(stats, embed_batch, dry_run)

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ä½¿ç”¨è€…ä¸­æ–·")
    except Exception as e:
        logger.error(f"\nâŒ Pipeline éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        http_client.close()

    # æœ€çµ‚çµ±è¨ˆ
    logger.info(stats.summary())

    if not dry_run:
        # é¡¯ç¤ºæœ€çµ‚ç‹€æ…‹
        result = supabase.table("health_articles").select("count", count="exact").execute()
        logger.info(f"ğŸ“š è³‡æ–™åº«æœ€çµ‚æ–‡ç« æ•¸: {result.count}")

    return stats


def main():
    parser = argparse.ArgumentParser(description="å¤§ç´€å…ƒå¥åº·æ–‡ç« å…¨è‡ªå‹•è™•ç† Pipeline")
    parser.add_argument("--batch-size", type=int, default=100, help="æ¯æ‰¹æŠ“å–æ–‡ç« æ•¸")
    parser.add_argument("--parse-batch", type=int, default=20, help="AI è§£ææ‰¹æ¬¡å¤§å°")
    parser.add_argument("--embed-batch", type=int, default=50, help="å‘é‡åŒ–æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--start-page", type=int, default=1, help="èµ·å§‹é ç¢¼")
    parser.add_argument("--max-articles", type=int, default=0, help="æœ€å¤§æ–‡ç« æ•¸ï¼ˆ0=ç„¡é™åˆ¶ï¼‰")
    parser.add_argument("--category", type=int, default=None, help="åˆ†é¡ç´¢å¼•ï¼ˆ0-2ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="è©¦é‹è¡Œæ¨¡å¼")

    args = parser.parse_args()

    run_full_pipeline(
        batch_size=args.batch_size,
        parse_batch=args.parse_batch,
        embed_batch=args.embed_batch,
        start_page=args.start_page,
        max_articles=args.max_articles,
        category_index=args.category,
        dry_run=args.dry_run,
    )


if __name__ == "__main__":
    main()
