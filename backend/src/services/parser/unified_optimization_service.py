"""Unified AI optimization service for Phase 7.

This service generates all AI optimization suggestions (title + SEO + FAQ)
in a single API call to save 40-60% cost and 30-40% time.

Cost comparison:
- Original (2 calls): ~$0.10-0.13 per article, 30-40s
- Unified (1 call): ~$0.06-0.08 per article, 20-30s
- Savings: 40-60% cost, 30-40% time
"""

import json
import logging
import re
from datetime import datetime
from decimal import Decimal
from typing import Any

from anthropic import AsyncAnthropic
from sqlalchemy.ext.asyncio import AsyncSession

from src.models.article import Article
from src.models.article_faq import ArticleFAQ
from src.models.seo_suggestions import SEOSuggestion
from src.models.title_suggestions import TitleSuggestion

logger = logging.getLogger(__name__)


class UnifiedOptimizationService:
    """ç»Ÿä¸€AIä¼˜åŒ–æœåŠ¡.

    ä¸€æ¬¡æ€§ç”Ÿæˆï¼š
    1. æ ‡é¢˜ä¼˜åŒ–å»ºè®®ï¼ˆ3æ®µå¼ï¼Œ2-3ä¸ªé€‰é¡¹ï¼‰
    2. SEOå…³é”®è¯ï¼ˆfocus/primary/secondaryï¼‰
    3. Meta Descriptionä¼˜åŒ–
    4. Tagsæ¨è
    5. FAQç”Ÿæˆï¼ˆ8-10ä¸ªï¼‰

    ä¼˜åŠ¿ï¼šèŠ‚çœTokenæˆæœ¬40-60%ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
    """

    def __init__(self, anthropic_client: AsyncAnthropic, db_session: AsyncSession):
        """Initialize service.

        Args:
            anthropic_client: Async Anthropic API client
            db_session: Async database session
        """
        self.client = anthropic_client
        self.db = db_session
        self.model = "claude-opus-4-5-20251101"
        self.max_tokens = 6000  # Increased for comprehensive response
        self.temperature = 0.35  # Balanced creativity

    async def generate_all_optimizations(
        self,
        article: Article,
        regenerate: bool = False,
    ) -> dict[str, Any]:
        """ä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰ä¼˜åŒ–å»ºè®®.

        Args:
            article: Article object with parsed content
            regenerate: Force regeneration even if suggestions exist

        Returns:
            {
                "title_suggestions": {...},
                "seo_suggestions": {...},
                "faqs": [...],
                "generation_metadata": {
                    "total_cost_usd": 0.07,
                    "total_tokens": 6500,
                    "duration_ms": 25000,
                    "savings_vs_separate": {...}
                }
            }

        Raises:
            ValueError: If article not parsed or prompt building fails
            RuntimeError: If AI API call fails
        """
        logger.info(f"Generating unified optimizations for article {article.id}")

        # Validation
        if not article.body_html and not article.body:
            raise ValueError(f"Article {article.id} has no content to optimize")

        # Check if already generated (unless regenerate=True)
        if not regenerate and article.unified_optimization_generated:
            logger.info(f"Article {article.id} already has optimizations, loading from cache")
            return await self._load_existing_optimizations(article.id)

        # Build unified prompt
        start_time = datetime.now()
        prompt = self._build_unified_prompt(article)

        # Call Claude API
        try:
            logger.info(f"Calling Claude API for article {article.id}")
            response = await self.client.messages.create(
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                messages=[{"role": "user", "content": prompt}],
            )

            # Track usage
            input_tokens = response.usage.input_tokens
            output_tokens = response.usage.output_tokens
            total_tokens = input_tokens + output_tokens

            # Calculate cost (Claude Sonnet 4.5 pricing: $3/M input, $15/M output)
            cost_usd = (input_tokens / 1_000_000 * 3.0) + (output_tokens / 1_000_000 * 15.0)

            duration_ms = int((datetime.now() - start_time).total_seconds() * 1000)

            logger.info(
                f"Article {article.id} - Claude response received: "
                f"{total_tokens} tokens, ${cost_usd:.4f}, {duration_ms}ms"
            )

        except Exception as e:
            logger.error(f"Claude API call failed for article {article.id}: {e}")
            raise RuntimeError(f"Failed to generate optimizations: {e}")

        # Parse response
        try:
            result = self._parse_unified_response(response.content[0].text)
        except Exception as e:
            logger.error(f"Failed to parse Claude response for article {article.id}: {e}")
            raise ValueError(f"AI response parsing failed: {e}")

        # Store to database
        try:
            await self._store_optimizations(article.id, result)
        except Exception as e:
            logger.error(f"Failed to store optimizations for article {article.id}: {e}")
            raise RuntimeError(f"Database storage failed: {e}")

        # Update article metadata
        article.unified_optimization_generated = True
        article.unified_optimization_generated_at = datetime.now()
        article.unified_optimization_cost = Decimal(str(cost_usd))
        await self.db.commit()

        # Build response with metadata
        return {
            "title_suggestions": result.get("title_suggestions", {}),
            "seo_suggestions": {
                "seo_keywords": result.get("seo_keywords", {}),
                "meta_description": result.get("meta_description", {}),
                "tags": result.get("tags", {}),
            },
            "faqs": result.get("faqs", []),
            "generation_metadata": {
                "total_cost_usd": round(cost_usd, 4),
                "total_tokens": total_tokens,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "duration_ms": duration_ms,
                "savings_vs_separate": self._calculate_savings(cost_usd, total_tokens, duration_ms),
            },
        }

    def _build_unified_prompt(self, article: Article) -> str:
        """æ„å»ºç»Ÿä¸€ä¼˜åŒ–Prompt.

        åŒ…å«5ä¸ªå­ä»»åŠ¡ï¼š
        1. æ ‡é¢˜ä¼˜åŒ–
        2. SEOå…³é”®è¯
        3. Meta Description
        4. Tagsæ¨è
        5. FAQç”Ÿæˆ
        """
        # Build full title from components
        full_title = self._build_full_title(article)

        # Get content for analysis (prioritize body_html)
        content = article.body_html or article.body or ""
        content_preview = content[:800] if len(content) > 800 else content

        # Build comprehensive prompt
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„SEOä¸“å®¶ã€å†…å®¹è¥é”€é¡¾é—®å’Œæ–‡æ¡ˆä¼˜åŒ–å¸ˆã€‚è¯·ä¸ºä»¥ä¸‹æ–‡ç« æä¾›**å…¨é¢çš„ä¼˜åŒ–å»ºè®®**ã€‚

## ğŸ“‹ æ–‡ç« ä¿¡æ¯

### ç»“æ„åŒ–æ•°æ®ï¼ˆå·²è§£æï¼‰

**æ ‡é¢˜ç»„ä»¶**:
- å‰ç¼€: {article.title_prefix or "ï¼ˆæ— ï¼‰"}
- ä¸»æ ‡é¢˜: {article.title_main or article.title}
- å‰¯æ ‡é¢˜: {article.title_suffix or "ï¼ˆæ— ï¼‰"}
- å®Œæ•´æ ‡é¢˜: {full_title}

**ä½œè€…**:
- ä½œè€…è¡Œ: {article.author_line or "ï¼ˆæ— ï¼‰"}
- ä½œè€…å: {article.author_name or "ï¼ˆæ— ï¼‰"}

**åˆæ­¥SEOæ•°æ®**ï¼ˆä»æ–‡æ¡£æå–ï¼‰:
- Meta Description: {article.meta_description or "ï¼ˆæ— ï¼‰"}
- å…³é”®è¯: {", ".join(article.seo_keywords[:10]) if article.seo_keywords else "ï¼ˆæ— ï¼‰"}
- æ ‡ç­¾: {", ".join(article.tags[:5]) if article.tags else "ï¼ˆæ— ï¼‰"}

**æ­£æ–‡å†…å®¹**ï¼ˆå‰800å­—ç¬¦ï¼‰:
{content_preview}...

---

## ğŸ¯ ä¼˜åŒ–ä»»åŠ¡

è¯·ä¸€æ¬¡æ€§å®Œæˆä»¥ä¸‹**5ä¸ªä¼˜åŒ–ä»»åŠ¡**ï¼š

---

### ä»»åŠ¡1: æ ‡é¢˜ä¼˜åŒ–ï¼ˆ3æ®µå¼ï¼‰ğŸ“

ä¸ºä¸‰æ®µå¼æ ‡é¢˜ç»“æ„ç”Ÿæˆ**2-3ä¸ª**ä¼˜åŒ–å»ºè®®ï¼š

```
å®Œæ•´æ ‡é¢˜ = [å‰ç¼€] | ä¸»æ ‡é¢˜ | [å‰¯æ ‡é¢˜]
```

**è¦æ±‚**:
1. ç”Ÿæˆ2-3ä¸ªä¸åŒé£æ ¼çš„æ ‡é¢˜æ–¹æ¡ˆ
2. å¿…é¡»åŒ…å«è‡³å°‘1ä¸ª**Data-Drivenå‹**ï¼ˆåŒ…å«å…·ä½“æ•°æ®ã€ç™¾åˆ†æ¯”ï¼‰
3. æ¨èåŒ…å«1ä¸ª**Authority-Backedå‹**ï¼ˆæƒå¨èƒŒä¹¦ï¼‰æˆ–**How-Toå‹**ï¼ˆæ“ä½œæŒ‡å—ï¼‰
4. å¯é€‰1ä¸ª**Comprehensive Guideå‹**ï¼ˆå…¨é¢æŒ‡å—ï¼‰æˆ–**Question-Basedå‹**ï¼ˆç–‘é—®å¼•å¯¼ï¼‰

**é•¿åº¦è§„èŒƒ**:
- å‰ç¼€: 2-6å­—ç¬¦ï¼ˆç®€çŸ­æœ‰åŠ›ï¼‰
- ä¸»æ ‡é¢˜: 15-30å­—ç¬¦ï¼ˆæ ¸å¿ƒå†…å®¹ï¼‰
- å‰¯æ ‡é¢˜: 4-12å­—ç¬¦ï¼ˆè¡¥å……ä¿¡æ¯ï¼‰
- å®Œæ•´æ ‡é¢˜: 25-50å­—ç¬¦ï¼ˆæ¨èï¼‰ï¼Œä¸è¶…è¿‡70å­—ç¬¦

**è¾“å‡ºæ ¼å¼**:
```json
"title_suggestions": {{
  "suggested_title_sets": [
    {{
      "id": "option_1",
      "title_prefix": "æ·±åº¦è§£æ",
      "title_main": "äººå·¥æ™ºèƒ½é©æ–°åŒ»ç–—è¯Šæ–­ï¼šå‡†ç¡®ç‡æå‡30%",
      "title_suffix": "æƒå¨æŒ‡å—",
      "full_title": "æ·±åº¦è§£æ | äººå·¥æ™ºèƒ½é©æ–°åŒ»ç–—è¯Šæ–­ï¼šå‡†ç¡®ç‡æå‡30% | æƒå¨æŒ‡å—",
      "score": 95,
      "strengths": ["åŒ…å«å…·ä½“æ•°æ®ï¼ˆ30%ï¼‰å¢å¼ºå¯ä¿¡åº¦", "ä½¿ç”¨åŠ¨ä½œè¯"é©æ–°"å¢å¼ºå¸å¼•åŠ›"],
      "type": "data_driven",
      "recommendation": "æœ€ä½³é€‰é¡¹ï¼Œå¹³è¡¡æ•°æ®ä¸å¸å¼•åŠ›",
      "character_count": {{"prefix": 4, "main": 22, "suffix": 4, "total": 34}}
    }},
    {{
      "id": "option_2",
      "title_prefix": "å®ç”¨æŒ‡å—",
      "title_main": "å¦‚ä½•è¿ç”¨AIæŠ€æœ¯æå‡åŒ»ç–—è¯Šæ–­æ•ˆç‡",
      "title_suffix": "ä¸“å®¶è§£è¯»",
      "full_title": "å®ç”¨æŒ‡å— | å¦‚ä½•è¿ç”¨AIæŠ€æœ¯æå‡åŒ»ç–—è¯Šæ–­æ•ˆç‡ | ä¸“å®¶è§£è¯»",
      "score": 88,
      "strengths": ["æ“ä½œæ€§å¼ºï¼Œæä¾›å®ç”¨ä»·å€¼", "ä¸“å®¶èƒŒä¹¦å¢å¼ºæƒå¨æ€§"],
      "type": "how_to",
      "recommendation": "é€‚åˆå¯»æ±‚å®ç”¨å»ºè®®çš„è¯»è€…",
      "character_count": {{"prefix": 4, "main": 18, "suffix": 4, "total": 30}}
    }}
  ],
  "optimization_notes": ["å»ºè®®åœ¨æ ‡é¢˜ä¸­çªå‡ºå…·ä½“æ•°æ®ä»¥å¢å¼ºå¯ä¿¡åº¦", "ä¿æŒæ ‡é¢˜ç®€æ´ï¼Œé¿å…è¶…è¿‡50å­—ç¬¦"],
  "seo_title_suggestions": {{
    "variants": [
      {{
        "id": "seo_variant_1",
        "seo_title": "2024å¹´AIé†«ç™‚å‰µæ–°è¶¨å‹¢",
        "reasoning": "èšç„¦æ ¸å¿ƒé—œéµå­—ã€ŒAIé†«ç™‚ã€å’Œã€Œå‰µæ–°ã€ï¼Œ30å­—å…§ï¼ŒåŒ…å«æ™‚æ•ˆæ€§",
        "keywords_focus": ["AIé†«ç™‚", "å‰µæ–°", "2024"],
        "character_count": 12
      }},
      {{
        "id": "seo_variant_2",
        "seo_title": "AIé†«ç™‚è¨ºæ–·æŠ€è¡“å…¨é¢è§£æ",
        "reasoning": "çªå‡ºã€Œè¨ºæ–·æŠ€è¡“ã€å’Œã€Œå…¨é¢è§£æã€ï¼Œå¸å¼•æ·±åº¦é–±è®€è€…",
        "keywords_focus": ["AIé†«ç™‚", "è¨ºæ–·æŠ€è¡“", "è§£æ"],
        "character_count": 12
      }}
    ],
    "original_seo_title": null,
    "notes": [
      "SEO Title å»ºè­°ä¿æŒåœ¨ 30 å­—ä»¥å…§",
      "åŒ…å«æ ¸å¿ƒé—œéµå­—ä»¥æå‡æœå°‹æ’å",
      "èˆ‡ H1 æ¨™é¡Œä¸»é¡Œä¸€è‡´ä½†æ›´ç²¾ç°¡"
    ]
  }}
}}
```

**æ³¨æ„**: `seo_title_suggestions` æ˜¯æ–°å¢æ¬„ä½ï¼Œç”¨æ–¼ç”Ÿæˆ SEO Titleï¼ˆ`<title>` æ¨™ç±¤ï¼‰å»ºè­°ï¼Œèˆ‡ H1 æ¨™é¡Œåˆ†é–‹ã€‚

**SEO Title vs H1 çš„å€åˆ¥**:
- **H1 æ¨™é¡Œ**: é é¢å…§å®¹çš„ä¸»æ¨™é¡Œï¼Œè¼ƒé•·ï¼ˆ25-50å­—ï¼‰ï¼Œçµ¦ç”¨æˆ¶é–±è®€
- **SEO Title**: æœå°‹å¼•æ“çµæœé¡¯ç¤ºçš„æ¨™é¡Œï¼Œè¼ƒçŸ­ï¼ˆ30å­—å·¦å³ï¼‰ï¼Œçµ¦æœå°‹å¼•æ“çœ‹

**SEO Title è¦æ±‚**:
1. ç”Ÿæˆ 2-3 å€‹ç²¾ç°¡è®Šé«”
2. é•·åº¦: **30å­—å·¦å³**ï¼ˆæœ€å¤š40å­—ï¼‰
3. å¿…é ˆåŒ…å«ä¸»é—œéµè©
4. å¯åŠ å…¥å¹´ä»½ã€æ•¸æ“šç­‰æå‡é»æ“Šç‡
5. èˆ‡ H1 ä¸»é¡Œä¸€è‡´ä½†æ›´ç²¾ç°¡

---

### ä»»åŠ¡2: SEOå…³é”®è¯åˆ†æğŸ”‘

æ·±åº¦åˆ†ææ–‡ç« å†…å®¹ï¼Œç”Ÿæˆä¸‰çº§å…³é”®è¯ä½“ç³»ï¼š

**è¦æ±‚**:
1. **Focus Keyword**ï¼ˆä¸»å…³é”®è¯ï¼‰: 1ä¸ªï¼Œæœç´¢é‡é«˜ã€ç«äº‰é€‚ä¸­ã€ä¸å†…å®¹é«˜åº¦ç›¸å…³
2. **Primary Keywords**ï¼ˆä¸»è¦å…³é”®è¯ï¼‰: 3-5ä¸ªï¼Œè¯­ä¹‰ç›¸å…³
3. **Secondary Keywords**ï¼ˆæ¬¡è¦å…³é”®è¯ï¼‰: 5-10ä¸ªï¼Œé•¿å°¾è¯

**è¾“å‡ºæ ¼å¼**:
```json
"seo_keywords": {{
  "focus_keyword": "äººå·¥æ™ºèƒ½åŒ»ç–—åº”ç”¨",
  "focus_keyword_rationale": "è¯¥è¯æœç´¢é‡é«˜ã€ç«äº‰ä¸­ç­‰ï¼Œä¸æ–‡ç« æ ¸å¿ƒå†…å®¹åŒ¹é…",
  "primary_keywords": ["AIè¯Šæ–­", "åŒ»ç–—å½±åƒåˆ†æ", "æ™ºèƒ½è¾…åŠ©è¯Šæ–­", "æ·±åº¦å­¦ä¹ åŒ»ç–—", "åŒ»ç–—AIæŠ€æœ¯"],
  "secondary_keywords": ["AIæ—©æœŸç­›æŸ¥", "åŒ»ç–—å¤§æ•°æ®åˆ†æ", "æ™ºèƒ½ç—…ç†è¯Šæ–­", "è¿œç¨‹AIè¯Šç–—", "ç²¾å‡†åŒ»ç–—AI", "åŒ»å­¦å½±åƒAIè¯†åˆ«", "ä¸´åºŠå†³ç­–æ”¯æŒç³»ç»Ÿ", "AIè¾…åŠ©æ‰‹æœ¯", "æ™ºèƒ½å¥åº·ç®¡ç†", "åŒ»ç–—æœºå™¨å­¦ä¹ "],
  "keyword_difficulty": {{"focus_keyword": 0.65, "average_difficulty": 0.52}},
  "search_volume_estimate": {{"focus_keyword": "5000-10000/æœˆ", "primary_keywords_total": "15000-25000/æœˆ"}}
}}
```

---

### ä»»åŠ¡3: Meta Descriptionä¼˜åŒ–ğŸ“„

åŸºäºæ–‡ç« å†…å®¹ç”Ÿæˆå¸å¼•ç‚¹å‡»çš„Meta Descriptionï¼š

**è¦æ±‚**:
1. é•¿åº¦: 150-160å­—ç¬¦
2. åŒ…å«Focus Keyword
3. å…·æœ‰å¸å¼•ç‚¹å‡»çš„å…ƒç´ ï¼ˆæ•°æ®ã€è¡ŒåŠ¨å·å¬ã€ç‹¬ç‰¹ä»·å€¼ï¼‰
4. å¦‚æœåŸæ–‡å·²æœ‰Meta Descriptionï¼Œè¿›è¡Œä¼˜åŒ–æ”¹è¿›

**è¾“å‡ºæ ¼å¼**:
```json
"meta_description": {{
  "original_meta_description": "æœ¬æ–‡ä»‹ç»AIåœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ã€‚",
  "suggested_meta_description": "æ·±å…¥è§£æAIå¦‚ä½•é©æ–°åŒ»ç–—è¯Šæ–­ï¼šä»å½±åƒåˆ†æåˆ°æ—©æœŸç­›æŸ¥ï¼Œäº†è§£äººå·¥æ™ºèƒ½å¦‚ä½•æå‡è¯Šæ–­å‡†ç¡®ç‡30%ä»¥ä¸Šï¼ŒåŠ©åŠ›ç²¾å‡†åŒ»ç–—å‘å±•ã€‚",
  "meta_description_improvements": [
    "æ·»åŠ å…·ä½“æ•°æ®ï¼ˆ30%ï¼‰å¢å¼ºå¯ä¿¡åº¦",
    "ä½¿ç”¨åŠ¨ä½œè¯"é©æ–°"ã€"æå‡"å¢å¼ºå¸å¼•åŠ›",
    "åŒ…å«ä¸»å…³é”®è¯"AIåŒ»ç–—è¯Šæ–­"",
    "ç¬¦åˆ150-160å­—ç¬¦æœ€ä½³é•¿åº¦"
  ],
  "meta_description_score": 92
}}
```

---

### ä»»åŠ¡4: Tagsæ¨èğŸ·ï¸

åˆ†æå†…å®¹åæ¨èç›¸å…³æ ‡ç­¾ï¼š

**è¦æ±‚**:
1. æ¨è6-8ä¸ªæ ‡ç­¾
2. åŒ…å«é«˜é¢‘æ ‡ç­¾ï¼ˆæµé‡å…¥å£ï¼‰+ ä¸­é¢‘æ ‡ç­¾ï¼ˆç²¾å‡†å®šä½ï¼‰+ é•¿å°¾æ ‡ç­¾ï¼ˆç»†åˆ†æµé‡ï¼‰
3. æ ‡ç­¾ä¸æ–‡ç« å†…å®¹é«˜åº¦ç›¸å…³
4. ä¼˜å…ˆæ¨èå¯èƒ½å·²å­˜åœ¨çš„å¸¸è§æ ‡ç­¾

**è¾“å‡ºæ ¼å¼**:
```json
"tags": {{
  "suggested_tags": [
    {{"tag": "äººå·¥æ™ºèƒ½", "relevance": 0.95, "type": "primary"}},
    {{"tag": "åŒ»ç–—AI", "relevance": 0.92, "type": "primary"}},
    {{"tag": "æ·±åº¦å­¦ä¹ åŒ»ç–—åº”ç”¨", "relevance": 0.78, "type": "secondary"}},
    {{"tag": "AIè¯Šæ–­å·¥å…·", "relevance": 0.85, "type": "trending"}},
    {{"tag": "åŒ»å­¦å½±åƒåˆ†æ", "relevance": 0.82, "type": "primary"}},
    {{"tag": "æ™ºèƒ½åŒ»ç–—", "relevance": 0.88, "type": "primary"}},
    {{"tag": "åŒ»ç–—åˆ›æ–°æŠ€æœ¯", "relevance": 0.75, "type": "secondary"}},
    {{"tag": "ç²¾å‡†åŒ»ç–—", "relevance": 0.80, "type": "secondary"}}
  ],
  "recommended_tag_count": "å»ºè®®ä½¿ç”¨6-8ä¸ªæ ‡ç­¾",
  "tag_strategy": "3ä¸ªé«˜é¢‘æ ‡ç­¾ + 3ä¸ªä¸­é¢‘æ ‡ç­¾ + 2ä¸ªé•¿å°¾æ ‡ç­¾"
}}
```

---

### ä»»åŠ¡5: FAQç”Ÿæˆï¼ˆAIæœç´¢ä¼˜åŒ–ï¼‰â“

æ ¹æ®æ–‡ç« å†…å®¹ç”Ÿæˆ**8-10ä¸ª**å¸¸è§é—®é¢˜å’Œç­”æ¡ˆï¼Œä¼˜åŒ–åœ¨AIæœç´¢å¼•æ“ä¸­çš„è¡¨ç°ï¼š

**è¦æ±‚**:
1. ç”Ÿæˆ8-10ä¸ªFAQ
2. é—®é¢˜ç±»å‹å¤šæ ·åŒ–ï¼šäº‹å®å‹ã€æ“ä½œå‹ã€å¯¹æ¯”å‹ã€å®šä¹‰å‹
3. é—®é¢˜ç¬¦åˆçœŸå®æœç´¢æ„å›¾ï¼ˆç”¨æˆ·åœ¨AIæœç´¢ä¸­ä¼šé—®çš„ï¼‰
4. ç­”æ¡ˆç®€æ´å‡†ç¡®ï¼ˆ50-150å­—ï¼‰ï¼ŒåŸºäºæ–‡ç« å†…å®¹ï¼Œä¸æœæ’°
5. è‡ªç„¶èå…¥ä¸»å…³é”®è¯å’Œç›¸å…³è¯

**è¾“å‡ºæ ¼å¼**:
```json
"faqs": [
  {{
    "question": "äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„å‡†ç¡®ç‡æœ‰å¤šé«˜ï¼Ÿ",
    "answer": "æ ¹æ®æœ€æ–°ç ”ç©¶ï¼ŒAIåŒ»ç–—è¯Šæ–­ç³»ç»Ÿåœ¨å½±åƒåˆ†æé¢†åŸŸçš„å‡†ç¡®ç‡å¯è¾¾95%ä»¥ä¸Šï¼Œéƒ¨åˆ†åœºæ™¯ç”šè‡³è¶…è¿‡äººç±»åŒ»ç”Ÿã€‚ä¾‹å¦‚åœ¨è‚ºç™Œæ—©æœŸç­›æŸ¥ä¸­ï¼ŒAIç³»ç»Ÿçš„å‡†ç¡®ç‡æ¯”ä¼ ç»Ÿæ–¹æ³•æå‡äº†30-40%ã€‚",
    "question_type": "factual",
    "search_intent": "informational",
    "keywords_covered": ["AIåŒ»ç–—è¯Šæ–­", "å‡†ç¡®ç‡", "å½±åƒåˆ†æ"],
    "confidence": 0.92
  }},
  {{
    "question": "åŒ»ç–—AIå¦‚ä½•ååŠ©åŒ»ç”Ÿè¿›è¡Œè¯Šæ–­ï¼Ÿ",
    "answer": "åŒ»ç–—AIé€šè¿‡åˆ†æåŒ»å­¦å½±åƒã€ç—…å†æ•°æ®å’Œæ£€éªŒç»“æœï¼Œä¸ºåŒ»ç”Ÿæä¾›è¾…åŠ©è¯Šæ–­å»ºè®®ã€‚ç³»ç»Ÿå¯ä»¥å¿«é€Ÿè¯†åˆ«ç—…å˜åŒºåŸŸã€æ ‡æ³¨å¼‚å¸¸æŒ‡æ ‡ï¼Œå¹¶ç»™å‡ºå¯èƒ½çš„è¯Šæ–­æ–¹å‘ï¼Œå¸®åŠ©åŒ»ç”Ÿæé«˜è¯Šæ–­æ•ˆç‡å’Œå‡†ç¡®æ€§ã€‚",
    "question_type": "how_to",
    "search_intent": "informational",
    "keywords_covered": ["åŒ»ç–—AI", "è¾…åŠ©è¯Šæ–­", "åŒ»å­¦å½±åƒ"],
    "confidence": 0.88
  }},
  {{
    "question": "AIè¯Šæ–­å’Œä¼ ç»Ÿè¯Šæ–­æ–¹æ³•æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
    "answer": "AIè¯Šæ–­ä¾é æ·±åº¦å­¦ä¹ ç®—æ³•å¤„ç†å¤§é‡åŒ»ç–—æ•°æ®ï¼Œå¯ä»¥7Ã—24å°æ—¶ä¸é—´æ–­å·¥ä½œï¼Œå¤„ç†é€Ÿåº¦å¿«ã€ä¸€è‡´æ€§é«˜ã€‚ä¼ ç»Ÿè¯Šæ–­ä¾èµ–åŒ»ç”Ÿç»éªŒï¼Œå—ä¸ªäººæ°´å¹³å’Œç–²åŠ³åº¦å½±å“ã€‚ä¸¤è€…ç»“åˆä½¿ç”¨æ•ˆæœæœ€ä½³ã€‚",
    "question_type": "comparison",
    "search_intent": "informational",
    "keywords_covered": ["AIè¯Šæ–­", "ä¼ ç»Ÿè¯Šæ–­", "æ·±åº¦å­¦ä¹ "],
    "confidence": 0.85
  }},
  {{
    "question": "ä»€ä¹ˆæ˜¯åŒ»å­¦å½±åƒAIè¯†åˆ«æŠ€æœ¯ï¼Ÿ",
    "answer": "åŒ»å­¦å½±åƒAIè¯†åˆ«æŠ€æœ¯æ˜¯æŒ‡åˆ©ç”¨è®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œè‡ªåŠ¨åˆ†æXå…‰ã€CTã€MRIç­‰åŒ»å­¦å›¾åƒï¼Œè¯†åˆ«ç—…å˜ç»„ç»‡ã€è‚¿ç˜¤ã€éª¨æŠ˜ç­‰å¼‚å¸¸æƒ…å†µçš„æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯å¯å¤§å¹…æé«˜è¯Šæ–­é€Ÿåº¦å’Œå‡†ç¡®ç‡ã€‚",
    "question_type": "definition",
    "search_intent": "informational",
    "keywords_covered": ["åŒ»å­¦å½±åƒ", "AIè¯†åˆ«", "è®¡ç®—æœºè§†è§‰"],
    "confidence": 0.90
  }},
  {{
    "question": "åŒ»ç–—AIæŠ€æœ¯ç›®å‰åº”ç”¨åœ¨å“ªäº›é¢†åŸŸï¼Ÿ",
    "answer": "åŒ»ç–—AIä¸»è¦åº”ç”¨äºï¼š1ï¼‰åŒ»å­¦å½±åƒè¯Šæ–­ï¼ˆè‚ºç™Œã€ä¹³è…ºç™Œç­›æŸ¥ï¼‰2ï¼‰ç—…ç†åˆ†æ 3ï¼‰è¯ç‰©ç ”å‘ 4ï¼‰æ‰‹æœ¯è¾…åŠ© 5ï¼‰å¥åº·ç®¡ç† 6ï¼‰è¿œç¨‹åŒ»ç–—ç­‰é¢†åŸŸã€‚å…¶ä¸­å½±åƒè¯Šæ–­æ˜¯æœ€æˆç†Ÿçš„åº”ç”¨æ–¹å‘ã€‚",
    "question_type": "factual",
    "search_intent": "informational",
    "keywords_covered": ["åŒ»ç–—AI", "åº”ç”¨é¢†åŸŸ", "å½±åƒè¯Šæ–­"],
    "confidence": 0.87
  }},
  {{
    "question": "ä½¿ç”¨AIè¿›è¡ŒåŒ»ç–—è¯Šæ–­å®‰å…¨å—ï¼Ÿ",
    "answer": "AIåŒ»ç–—è¯Šæ–­ç³»ç»Ÿç»è¿‡å¤§é‡æ•°æ®è®­ç»ƒå’Œä¸´åºŠéªŒè¯ï¼Œå®‰å…¨æ€§è¾ƒé«˜ã€‚ä½†ç›®å‰ä¸»è¦ä½œä¸ºè¾…åŠ©å·¥å…·ï¼Œæœ€ç»ˆè¯Šæ–­å†³ç­–ä»éœ€ç”±ä¸“ä¸šåŒ»ç”Ÿåšå‡ºã€‚ç›‘ç®¡æœºæ„å¯¹åŒ»ç–—AIäº§å“æœ‰ä¸¥æ ¼çš„è®¤è¯æ ‡å‡†ã€‚",
    "question_type": "factual",
    "search_intent": "informational",
    "keywords_covered": ["AIè¯Šæ–­", "å®‰å…¨æ€§", "ä¸´åºŠéªŒè¯"],
    "confidence": 0.83
  }},
  {{
    "question": "ä¸ªäººåŒ»ç–—æ•°æ®åœ¨AIç³»ç»Ÿä¸­å¦‚ä½•ä¿æŠ¤ï¼Ÿ",
    "answer": "åŒ»ç–—AIç³»ç»Ÿé‡‡ç”¨æ•°æ®åŠ å¯†ã€å»æ ‡è¯†åŒ–ã€è®¿é—®æ§åˆ¶ç­‰æŠ€æœ¯ä¿æŠ¤æ‚£è€…éšç§ã€‚æ•°æ®å¤„ç†éµå¾ªGDPRã€HIPAAç­‰æ³•è§„è¦æ±‚ã€‚æ­£è§„åŒ»ç–—æœºæ„ä¼šç­¾ç½²éšç§ä¿æŠ¤åè®®ï¼Œç¡®ä¿æ•°æ®å®‰å…¨ã€‚",
    "question_type": "how_to",
    "search_intent": "informational",
    "keywords_covered": ["åŒ»ç–—æ•°æ®", "éšç§ä¿æŠ¤", "æ•°æ®å®‰å…¨"],
    "confidence": 0.80
  }},
  {{
    "question": "åŒ»ç–—AIæœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
    "answer": "æœªæ¥åŒ»ç–—AIå°†å‘å¤šæ¨¡æ€èåˆï¼ˆæ•´åˆå½±åƒã€åŸºå› ã€ç—…å†ç­‰å¤šæºæ•°æ®ï¼‰ã€ä¸ªæ€§åŒ–åŒ»ç–—ã€å®æ—¶è¯Šæ–­ã€è¿œç¨‹æ™ºèƒ½åŒ»ç–—ç­‰æ–¹å‘å‘å±•ã€‚é¢„è®¡2030å¹´AIå°†è¦†ç›–80%ä»¥ä¸Šçš„å¸¸è§„è¯Šæ–­åœºæ™¯ã€‚",
    "question_type": "factual",
    "search_intent": "informational",
    "keywords_covered": ["åŒ»ç–—AI", "å‘å±•è¶‹åŠ¿", "ä¸ªæ€§åŒ–åŒ»ç–—"],
    "confidence": 0.86
  }},
  {{
    "question": "å¦‚ä½•é€‰æ‹©åˆé€‚çš„åŒ»ç–—AIè¯Šæ–­å·¥å…·ï¼Ÿ",
    "answer": "é€‰æ‹©åŒ»ç–—AIå·¥å…·åº”è€ƒè™‘ï¼š1ï¼‰æ˜¯å¦è·å¾—ç›‘ç®¡æœºæ„è®¤è¯ 2ï¼‰ä¸´åºŠéªŒè¯æ•°æ®æ˜¯å¦å……åˆ† 3ï¼‰å‡†ç¡®ç‡å’Œç‰¹å¼‚æ€§æŒ‡æ ‡ 4ï¼‰é€‚ç”¨ç—…ç§èŒƒå›´ 5ï¼‰æŠ€æœ¯æ”¯æŒå’Œæ›´æ–°é¢‘ç‡ã€‚å»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç–—æœºæ„æ¨èã€‚",
    "question_type": "how_to",
    "search_intent": "transactional",
    "keywords_covered": ["åŒ»ç–—AIå·¥å…·", "é€‰æ‹©æ ‡å‡†", "è®¤è¯"],
    "confidence": 0.82
  }},
  {{
    "question": "åŒ»ç”Ÿéœ€è¦å­¦ä¹ AIæŠ€æœ¯çŸ¥è¯†å—ï¼Ÿ",
    "answer": "åŒ»ç”Ÿä¸éœ€è¦æ·±å…¥æŒæ¡AIç®—æ³•ï¼Œä½†åº”äº†è§£AIå·¥å…·çš„åŸºæœ¬åŸç†ã€é€‚ç”¨åœºæ™¯å’Œå±€é™æ€§ï¼Œä»¥ä¾¿æ­£ç¡®ä½¿ç”¨å’Œè§£è¯»AIè¯Šæ–­ç»“æœã€‚å¾ˆå¤šåŒ»å­¦é™¢å·²å°†AIåŒ»ç–—ç›¸å…³è¯¾ç¨‹çº³å…¥åŸ¹è®­ä½“ç³»ã€‚",
    "question_type": "factual",
    "search_intent": "informational",
    "keywords_covered": ["åŒ»ç”Ÿ", "AIçŸ¥è¯†", "åŒ»å­¦æ•™è‚²"],
    "confidence": 0.78
  }}
]
```

---

## ğŸ“¤ æœ€ç»ˆè¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSON Schemaè¾“å‡ºæ‰€æœ‰5ä¸ªä»»åŠ¡çš„ç»“æœï¼š

```json
{{
  "title_suggestions": {{
    "suggested_title_sets": [...],
    "optimization_notes": [...]
  }},
  "seo_keywords": {{
    "focus_keyword": "...",
    "primary_keywords": [...],
    "secondary_keywords": [...],
    ...
  }},
  "meta_description": {{
    "suggested_meta_description": "...",
    "meta_description_improvements": [...],
    ...
  }},
  "tags": {{
    "suggested_tags": [...],
    ...
  }},
  "faqs": [
    {{"question": "...", "answer": "...", ...}},
    // ... 8-10ä¸ªFAQ
  ]
}}
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

1. **å†…å®¹ä¸€è‡´æ€§**: æ ‡é¢˜ã€å…³é”®è¯ã€Metaã€Tagsã€FAQåº”ç›¸äº’åè°ƒï¼Œä½¿ç”¨ç»Ÿä¸€çš„æ ¸å¿ƒæ¦‚å¿µ
2. **å…³é”®è¯è¦†ç›–**: ç¡®ä¿Focus Keywordåœ¨æ ‡é¢˜ã€Meta Descriptionã€FAQä¸­éƒ½æœ‰å‡ºç°
3. **æ•°æ®å‡†ç¡®**: FAQç­”æ¡ˆå¿…é¡»åŸºäºæ–‡ç« å†…å®¹ï¼Œä¸å¾—æœæ’°æ•°æ®
4. **é•¿åº¦æ§åˆ¶**: ä¸¥æ ¼éµå®ˆå„é¡¹é•¿åº¦é™åˆ¶
5. **å¤šæ ·æ€§**: æ ‡é¢˜ç±»å‹å¤šæ ·ã€FAQé—®é¢˜ç±»å‹å¤šæ ·

---

ç°åœ¨è¯·å®Œæˆæ‰€æœ‰5ä¸ªä¼˜åŒ–ä»»åŠ¡ã€‚"""

        return prompt

    def _build_full_title(self, article: Article) -> str:
        """æ„å»ºå®Œæ•´æ ‡é¢˜."""
        parts = []
        if article.title_prefix:
            parts.append(article.title_prefix)

        # Use title_main if available, otherwise fall back to title
        main_title = article.title_main or article.title
        parts.append(main_title)

        if article.title_suffix:
            parts.append(article.title_suffix)

        return " | ".join(parts)

    def _parse_unified_response(self, response_text: str) -> dict[str, Any]:
        """è§£æAIå“åº”.

        æå–ï¼š
        - title_suggestions
        - seo_keywords
        - meta_description
        - tags
        - faqs
        """
        # Try to extract JSON from response
        # Pattern 1: Look for JSON code block
        json_match = re.search(r"```json\s*(\{.*?\})\s*```", response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # Pattern 2: Try to find raw JSON object
            json_match = re.search(r"\{.*\}", response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
            else:
                raise ValueError("No JSON found in AI response")

        try:
            data = json.loads(json_str)
            logger.info("Successfully parsed AI response JSON")
            return data
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response JSON: {e}")
            logger.debug(f"Response text (first 500 chars): {response_text[:500]}")
            raise ValueError(f"AI response JSON parsing failed: {e}")

    async def _store_optimizations(self, article_id: int, result: dict[str, Any]) -> None:
        """åˆ†åˆ«å­˜å‚¨ä¼˜åŒ–ç»“æœåˆ°å¯¹åº”çš„è¡¨.

        Args:
            article_id: Article ID
            result: AIç”Ÿæˆçš„å®Œæ•´ç»“æœ
        """
        logger.info(f"Storing optimizations for article {article_id}")

        # Get article to access original title components
        from sqlalchemy import select

        stmt = select(Article).where(Article.id == article_id)
        db_result = await self.db.execute(stmt)
        article = db_result.scalar_one()

        # 1. å­˜å‚¨æ ‡é¢˜å»ºè®®åˆ° title_suggestions è¡¨
        title_data = result.get("title_suggestions", {})
        await self._save_title_suggestions(article_id, article, title_data)

        # 2. å­˜å‚¨SEOå»ºè®®åˆ° seo_suggestions è¡¨
        seo_keywords = result.get("seo_keywords", {})
        meta_desc = result.get("meta_description", {})
        tags_data = result.get("tags", {})

        await self._save_seo_suggestions(article_id, seo_keywords, meta_desc, tags_data)

        # 3. å­˜å‚¨FAQåˆ° article_faqs è¡¨
        faqs = result.get("faqs", [])
        await self._save_faqs(article_id, faqs)

        await self.db.commit()
        logger.info(f"Successfully stored all optimizations for article {article_id}")

    async def _save_title_suggestions(
        self, article_id: int, article: Article, data: dict
    ) -> None:
        """å­˜å‚¨æ ‡é¢˜å»ºè®®ï¼ˆåŒ…å« H1 å’Œ SEO Titleï¼‰."""
        # Check if already exists
        from sqlalchemy import select

        stmt = select(TitleSuggestion).where(TitleSuggestion.article_id == article_id)
        result = await self.db.execute(stmt)
        existing = result.scalar_one_or_none()

        # Prepare SEO title suggestions data
        seo_title_suggestions = data.get("seo_title_suggestions", {})

        # Update original_seo_title in suggestions if article has extracted seo_title
        if article.seo_title and article.seo_title_extracted:
            if "original_seo_title" not in seo_title_suggestions or not seo_title_suggestions["original_seo_title"]:
                seo_title_suggestions["original_seo_title"] = article.seo_title

        if existing:
            # Update existing
            existing.suggested_title_sets = data.get("suggested_title_sets", [])
            existing.optimization_notes = data.get("optimization_notes", [])
            existing.suggested_seo_titles = seo_title_suggestions if seo_title_suggestions else None
            existing.generated_at = datetime.now()
        else:
            # Create new
            title_suggestion = TitleSuggestion(
                article_id=article_id,
                original_title_prefix=article.title_prefix,
                original_title_main=article.title_main or article.title,
                original_title_suffix=article.title_suffix,
                suggested_title_sets=data.get("suggested_title_sets", []),
                optimization_notes=data.get("optimization_notes", []),
                suggested_seo_titles=seo_title_suggestions if seo_title_suggestions else None,
            )
            self.db.add(title_suggestion)

        logger.info(f"Saved title suggestions (H1 + SEO) for article {article_id}")

    async def _save_seo_suggestions(
        self,
        article_id: int,
        seo_keywords: dict,
        meta_desc: dict,
        tags_data: dict,
    ) -> None:
        """å­˜å‚¨SEOå»ºè®®."""
        from sqlalchemy import select

        stmt = select(SEOSuggestion).where(SEOSuggestion.article_id == article_id)
        result = await self.db.execute(stmt)
        existing = result.scalar_one_or_none()

        if existing:
            # Update existing
            existing.focus_keyword = seo_keywords.get("focus_keyword")
            existing.focus_keyword_rationale = seo_keywords.get("focus_keyword_rationale")
            existing.primary_keywords = seo_keywords.get("primary_keywords", [])
            existing.secondary_keywords = seo_keywords.get("secondary_keywords", [])
            existing.keyword_difficulty = seo_keywords.get("keyword_difficulty")
            existing.search_volume_estimate = seo_keywords.get("search_volume_estimate")
            existing.suggested_meta_description = meta_desc.get("suggested_meta_description")
            existing.meta_description_improvements = meta_desc.get(
                "meta_description_improvements", []
            )
            existing.meta_description_score = meta_desc.get("meta_description_score")
            existing.suggested_tags = tags_data.get("suggested_tags", [])
            existing.tag_strategy = tags_data.get("tag_strategy")
            existing.generated_at = datetime.now()
        else:
            # Create new
            seo_suggestion = SEOSuggestion(
                article_id=article_id,
                focus_keyword=seo_keywords.get("focus_keyword"),
                focus_keyword_rationale=seo_keywords.get("focus_keyword_rationale"),
                primary_keywords=seo_keywords.get("primary_keywords", []),
                secondary_keywords=seo_keywords.get("secondary_keywords", []),
                keyword_difficulty=seo_keywords.get("keyword_difficulty"),
                search_volume_estimate=seo_keywords.get("search_volume_estimate"),
                suggested_meta_description=meta_desc.get("suggested_meta_description"),
                meta_description_improvements=meta_desc.get("meta_description_improvements", []),
                meta_description_score=meta_desc.get("meta_description_score"),
                suggested_tags=tags_data.get("suggested_tags", []),
                tag_strategy=tags_data.get("tag_strategy"),
            )
            self.db.add(seo_suggestion)

        logger.info(f"Saved SEO suggestions for article {article_id}")

    async def _save_faqs(self, article_id: int, faqs: list[dict]) -> None:
        """å­˜å‚¨FAQ."""
        from sqlalchemy import delete

        # Delete existing FAQs for this article
        stmt = delete(ArticleFAQ).where(ArticleFAQ.article_id == article_id)
        await self.db.execute(stmt)

        # Create new FAQs
        for position, faq_data in enumerate(faqs):
            # Use string values directly (matching varchar columns in database)
            question_type = faq_data.get("question_type", "factual")
            if question_type not in ("factual", "how_to", "comparison", "definition"):
                question_type = "factual"

            search_intent = faq_data.get("search_intent", "informational")
            if search_intent not in ("informational", "navigational", "transactional"):
                search_intent = "informational"

            faq = ArticleFAQ(
                article_id=article_id,
                question=faq_data.get("question", ""),
                answer=faq_data.get("answer", ""),
                question_type=question_type,
                search_intent=search_intent,
                keywords_covered=faq_data.get("keywords_covered", []),
                confidence=faq_data.get("confidence"),
                position=position,
                status="draft",  # String value instead of enum
            )
            self.db.add(faq)

        logger.info(f"Saved {len(faqs)} FAQs for article {article_id}")

    async def _load_existing_optimizations(self, article_id: int) -> dict[str, Any]:
        """Load existing optimizations from database."""
        from sqlalchemy import select

        # Load title suggestions
        stmt = select(TitleSuggestion).where(TitleSuggestion.article_id == article_id)
        result = await self.db.execute(stmt)
        title_suggestion = result.scalar_one_or_none()

        # Load SEO suggestions
        stmt = select(SEOSuggestion).where(SEOSuggestion.article_id == article_id)
        result = await self.db.execute(stmt)
        seo_suggestion = result.scalar_one_or_none()

        # Load FAQs
        stmt = select(ArticleFAQ).where(ArticleFAQ.article_id == article_id).order_by(ArticleFAQ.position)
        result = await self.db.execute(stmt)
        faqs = result.scalars().all()

        return {
            "title_suggestions": {
                "suggested_title_sets": title_suggestion.suggested_title_sets if title_suggestion else [],
                "optimization_notes": title_suggestion.optimization_notes if title_suggestion else [],
                "seo_title_suggestions": title_suggestion.suggested_seo_titles if title_suggestion else {},
            },
            "seo_suggestions": {
                "seo_keywords": {
                    "focus_keyword": seo_suggestion.focus_keyword if seo_suggestion else None,
                    "focus_keyword_rationale": seo_suggestion.focus_keyword_rationale if seo_suggestion else None,
                    "primary_keywords": seo_suggestion.primary_keywords if seo_suggestion else [],
                    "secondary_keywords": seo_suggestion.secondary_keywords if seo_suggestion else [],
                },
                "meta_description": {
                    "suggested_meta_description": seo_suggestion.suggested_meta_description if seo_suggestion else None,
                    "meta_description_improvements": seo_suggestion.meta_description_improvements if seo_suggestion else [],
                    "meta_description_score": seo_suggestion.meta_description_score if seo_suggestion else None,
                },
                "tags": {
                    "suggested_tags": seo_suggestion.suggested_tags if seo_suggestion else [],
                    "tag_strategy": seo_suggestion.tag_strategy if seo_suggestion else None,
                },
            },
            "faqs": [
                {
                    "question": faq.question,
                    "answer": faq.answer,
                    "question_type": faq.question_type,  # Already a string
                    "search_intent": faq.search_intent,  # Already a string
                    "keywords_covered": faq.keywords_covered or [],
                    "confidence": float(faq.confidence) if faq.confidence else None,
                }
                for faq in faqs
            ],
            "generation_metadata": {
                "cached": True,
                "message": "Loaded from cache (no AI call)",
            },
        }

    def _calculate_savings(self, cost_usd: float, total_tokens: int, duration_ms: int) -> dict:
        """Calculate savings vs separate API calls."""
        # Estimated cost/time for separate calls (original approach)
        # Step 1 Title: ~2,700 tokens, $0.02-0.03, 10-15s
        # Step 3 SEO+FAQ: ~5,500 tokens, $0.08-0.10, 20-25s
        original_tokens = 8200
        original_cost = 0.115  # Average of $0.10-0.13
        original_duration_ms = 35000  # Average of 30-40s

        saved_tokens = original_tokens - total_tokens
        saved_cost = original_cost - cost_usd
        saved_time_ms = original_duration_ms - duration_ms

        return {
            "original_tokens": original_tokens,
            "original_cost_usd": round(original_cost, 4),
            "original_duration_ms": original_duration_ms,
            "saved_tokens": saved_tokens,
            "saved_cost_usd": round(saved_cost, 4),
            "saved_duration_ms": saved_time_ms,
            "cost_savings_percentage": round((saved_cost / original_cost) * 100, 1),
            "time_savings_percentage": round((saved_time_ms / original_duration_ms) * 100, 1),
        }
