# Pipeline Issues Analysis - Critical Bugs in Parsing Flow

## Date: 2025-11-12
## Report by: Codex CLI + Claude Analysis

---

## ğŸš¨ Executive Summary

**Status**: âœ… **ISSUES CONFIRMED - Both P1 issues are VALID and require immediate fixes**

Codex CLI identified two critical bugs in the worklist pipeline that make the new parsing stage ineffective:

1. **[P1] Parsing failures don't stop proofreading** - Breaks the review gate
2. **[P1] Parser receives cleaned text instead of HTML** - Images and structure lost

Both issues are **confirmed** and **actively breaking** the parsing functionality.

---

## ğŸ” Issue #1: Skip Proofreading When Parsing Fails

### Problem Statement

**Location**: `backend/src/services/worklist/pipeline.py:47-55, 94-210`

**Current Behavior**:
```python
async def process_new_item(self, item: WorklistItem) -> None:
    """Ensure article exists, run parsing, then proofreading."""
    article = await self._ensure_article(item)

    # Step 1: Parse document
    await self._run_parsing(item)  # âŒ NEVER signals failure

    # Step 2: Run proofreading - ALWAYS executes
    await self._run_proofreading(item, article)  # âŒ Runs even if parsing failed
```

**Root Cause Analysis**:

`_run_parsing()` has **three code paths** but **none propagate failure upward**:

#### Path 1: Parsing Fails (Lines 110-127)
```python
if not parsing_result.success:
    # Parsing failed
    logger.error("worklist_parsing_failed", ...)
    item.mark_status(WorklistStatus.PARSING)  # âœ… Correct status
    item.add_note({"message": "AIè§£æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨å®¡æ ¸", ...})
    self.session.add(item)
    return  # âŒ Returns but doesn't signal failure
```

**Analysis**:
- âœ… Sets status to `PARSING` (waiting for manual review)
- âŒ **Returns without raising** â†’ `process_new_item` continues
- âŒ **No return value** â†’ Caller can't detect failure

#### Path 2: Parsing Succeeds (Lines 129-192)
```python
# Parsing succeeded
parsed_article = parsing_result.parsed_article
# ... update worklist item with parsed data ...
item.mark_status(WorklistStatus.PARSING_REVIEW)  # âœ… Correct
item.add_note({"message": "AIè§£æå®Œæˆï¼Œç­‰å¾…äººå·¥å®¡æ ¸è§£æç»“æœ", ...})
self.session.add(item)
# âŒ No explicit return value (implicit None)
```

**Analysis**:
- âœ… Sets status to `PARSING_REVIEW` (waiting for human approval)
- âŒ **No return value** â†’ Success not communicated

#### Path 3: Exception Thrown (Lines 194-209)
```python
except Exception as exc:
    logger.error("worklist_parsing_exception", ...)
    item.mark_status(WorklistStatus.PARSING)  # âœ… Correct status
    item.add_note({"message": "è§£æè¿‡ç¨‹å¼‚å¸¸ï¼Œéœ€è¦é‡è¯•", ...})
    self.session.add(item)
    # âŒ Swallows exception, doesn't re-raise
```

**Analysis**:
- âœ… Sets status to `PARSING` (needs retry)
- âŒ **Exception swallowed** â†’ No failure signal
- âŒ **No return value**

### Impact

**Severity**: ğŸ”´ **CRITICAL**

1. **Review Gate Broken**: Items that fail parsing still advance to `proofreading_review`
2. **Status Confusion**: `_run_proofreading` overwrites status from `PARSING` â†’ `PROOFREADING_REVIEW`
3. **Operator Confusion**: Failed parsing items appear as "successfully proofread"
4. **Data Loss**: Parsing failures masked by proofreading execution
5. **Wasted API Costs**: Proofreading runs on unparsed content

**Example Flow (Current - BROKEN)**:
```
1. Parsing fails â†’ status = PARSING
2. process_new_item continues (no failure signal)
3. _run_proofreading executes
4. Status overwritten â†’ status = PROOFREADING_REVIEW
5. Article marked as IN_REVIEW
6. âŒ Parsing failure completely masked
```

### Verification

I reviewed the code at:
- `pipeline.py:47-55` - `process_new_item` unconditionally calls both functions
- `pipeline.py:94-210` - `_run_parsing` never returns success/failure boolean
- `pipeline.py:211+` - `_run_proofreading` always executes

**Confirmed**: âœ… **Issue exists exactly as described**

---

## ğŸ” Issue #2: Parser Receives Cleaned Text, Not HTML

### Problem Statement

**Location**:
- `backend/src/services/worklist/pipeline.py:94-109`
- `backend/src/services/google_drive/sync_service.py:256-336`

**Current Flow**:

```
1. Google Drive Sync
   â”œâ”€ Download HTML: _export_google_doc(..., "text/html")  âœ… Has images
   â”œâ”€ Parse HTML: _parse_html_content(html_content)
   â”‚  â””â”€ GoogleDocsHTMLParser strips all <img> tags  âŒ
   â”œâ”€ Result: cleaned_text (Markdown-like, no HTML)
   â””â”€ Store: WorklistItem.content = cleaned_text  âŒ

2. Pipeline Parsing
   â”œâ”€ Read: raw_html = item.content  âŒ Actually cleaned text
   â””â”€ Parse: parser_service.parse_document(raw_html)  âŒ No images to find
```

### Root Cause Analysis

#### Step 1: HTML Export (sync_service.py:256)
```python
html_content = await self._export_google_doc(storage, file_id, "text/html")
```
âœ… **Correct**: Downloads full HTML with `<img>` tags, styles, structure

#### Step 2: HTML Cleaning (sync_service.py:268)
```python
# Parse and clean the HTML
content, parsing_status = self._parse_html_content(html_content)
```

**Inside `_parse_html_content` (lines 353-379)**:
```python
def _parse_html_content(self, html_content: str) -> tuple[str, Any]:
    parser = GoogleDocsHTMLParser()  # âŒ Strips images
    parser.feed(html_content)
    cleaned_text = parser.get_clean_text()  # âŒ Returns plain text
    return cleaned_text, status
```

**What `GoogleDocsHTMLParser` does**:
- âœ… Removes Google Docs CSS/styles
- âœ… Extracts text content
- âŒ **Strips all `<img>` tags and src URLs**
- âŒ **Removes HTML structure** (headers, lists, etc.)
- âŒ **Loses formatting cues**

#### Step 3: Storage (sync_service.py:328)
```python
parsed = self._parse_document_content(content, file_name=file_name)
# content is already cleaned text (no HTML)
```

**Stored in database**:
```python
WorklistItem(
    content=content,  # âŒ Cleaned text, not HTML
    ...
)
```

#### Step 4: Pipeline Parsing (pipeline.py:98)
```python
async def _run_parsing(self, item: WorklistItem) -> None:
    raw_html = item.content  # âŒ Not HTML - it's cleaned text!

    parsing_result = self.parser_service.parse_document(raw_html)
    # ArticleParserService expects HTML with <img> tags
    # But receives plain text â†’ no images found
```

### Impact

**Severity**: ğŸ”´ **CRITICAL**

1. **Images Always Empty**: Parser can't find `<img>` tags in cleaned text
2. **Structure Lost**: No `<h1>`, `<h2>`, `<p>` tags for heuristic parsing
3. **AI Prompting Wrong**: AI receives sanitized text instead of rich HTML
4. **Metadata Extraction Fails**: Title/author/SEO extraction relies on HTML structure
5. **Parsing Requirements Unmet**: New parsing stage can't satisfy its design goals

**Example**:

**Original Google Docs HTML**:
```html
<h1>ã€å¥åº·ã€‘å¦‚ä½•æå‡å…ç–«åŠ›</h1>
<p>ä½œè€…ï¼šå¼µé†«å¸«</p>
<img src="https://lh3.googleusercontent.com/..." alt="å…ç–«ç³»çµ±ç¤ºæ„åœ–">
<p>æ–‡ç« å…§å®¹...</p>
```

**Cleaned Text (Stored in `WorklistItem.content`)**:
```
ã€å¥åº·ã€‘å¦‚ä½•æå‡å…ç–«åŠ›
ä½œè€…ï¼šå¼µé†«å¸«
æ–‡ç« å…§å®¹...
```
âŒ **Image URL lost completely**

**Parser Receives**:
```python
# parse_document() gets:
raw_html = "ã€å¥åº·ã€‘å¦‚ä½•æå‡å…ç–«åŠ›\nä½œè€…ï¼šå¼µé†«å¸«\næ–‡ç« å…§å®¹..."
# No <img>, no <h1>, no structure â†’ Parsing fails
```

### Verification

I traced the complete data flow:

1. âœ… **Confirmed**: `sync_service.py:256` exports HTML with images
2. âœ… **Confirmed**: `sync_service.py:268` strips HTML to text
3. âœ… **Confirmed**: `pipeline.py:98` receives cleaned text, not HTML
4. âœ… **Confirmed**: ArticleParserService expects HTML structure

**Issue Confirmed**: âœ… **Exactly as described**

---

## ğŸ› ï¸ Required Fixes

### Fix #1: Make `_run_parsing` Return Success/Failure

**Option A: Return Boolean (Recommended)**
```python
async def _run_parsing(self, item: WorklistItem) -> bool:
    """Parse document content.

    Returns:
        True if parsing succeeded and item is ready for proofreading
        False if parsing failed or needs manual review
    """
    try:
        raw_html = item.content
        parsing_result = self.parser_service.parse_document(raw_html)

        if not parsing_result.success:
            # Parsing failed
            logger.error("worklist_parsing_failed", ...)
            item.mark_status(WorklistStatus.PARSING)
            item.add_note({"message": "AIè§£æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨å®¡æ ¸", ...})
            self.session.add(item)
            return False  # âœ… Signal failure

        # Parsing succeeded
        parsed_article = parsing_result.parsed_article
        # ... update worklist item ...
        item.mark_status(WorklistStatus.PARSING_REVIEW)
        item.add_note({"message": "AIè§£æå®Œæˆï¼Œç­‰å¾…äººå·¥å®¡æ ¸", ...})
        self.session.add(item)
        return True  # âœ… Signal success (but needs review)

    except Exception as exc:
        logger.error("worklist_parsing_exception", ...)
        item.mark_status(WorklistStatus.PARSING)
        item.add_note({"message": "è§£æè¿‡ç¨‹å¼‚å¸¸ï¼Œéœ€è¦é‡è¯•", ...})
        self.session.add(item)
        return False  # âœ… Signal failure

async def process_new_item(self, item: WorklistItem) -> None:
    """Ensure article exists, run parsing, then proofreading."""
    article = await self._ensure_article(item)

    # Step 1: Parse document
    parsing_success = await self._run_parsing(item)

    # Step 2: Only run proofreading if parsing succeeded
    if parsing_success:
        await self._run_proofreading(item, article)
    else:
        logger.info(
            "worklist_skipped_proofreading",
            worklist_id=item.id,
            reason="parsing_failed_or_needs_review"
        )
```

**Option B: Raise Exception (Alternative)**
```python
async def _run_parsing(self, item: WorklistItem) -> None:
    """Parse document content. Raises ParsingFailedError on failure."""
    try:
        raw_html = item.content
        parsing_result = self.parser_service.parse_document(raw_html)

        if not parsing_result.success:
            item.mark_status(WorklistStatus.PARSING)
            item.add_note({"message": "AIè§£æå¤±è´¥ï¼Œéœ€è¦æ‰‹åŠ¨å®¡æ ¸", ...})
            self.session.add(item)
            raise ParsingFailedError("Parsing failed, needs manual review")

        # ... rest of success path ...

    except ParsingFailedError:
        raise  # Re-raise to caller
    except Exception as exc:
        item.mark_status(WorklistStatus.PARSING)
        item.add_note({"message": "è§£æè¿‡ç¨‹å¼‚å¸¸", ...})
        self.session.add(item)
        raise ParsingFailedError(f"Parsing exception: {exc}") from exc

async def process_new_item(self, item: WorklistItem) -> None:
    article = await self._ensure_article(item)

    try:
        await self._run_parsing(item)
        # Only reached if parsing succeeded
        await self._run_proofreading(item, article)
    except ParsingFailedError as e:
        logger.info("worklist_parsing_failed", worklist_id=item.id, reason=str(e))
        # Don't run proofreading
```

**Recommendation**: Use **Option A (boolean return)** - simpler, more explicit

---

### Fix #2: Store and Use Raw HTML

**Strategy**: Store both raw HTML and cleaned text

#### Part 1: Update `WorklistItem` Model

```python
# backend/src/models/worklist.py
class WorklistItem(Base):
    __tablename__ = "worklist_items"

    content = Column(Text)  # Keep for backward compatibility (cleaned text)
    raw_html = Column(Text, nullable=True)  # NEW: Store original HTML
```

**Migration Required**:
```python
# alembic/versions/xxx_add_raw_html_to_worklist.py
def upgrade():
    op.add_column('worklist_items', sa.Column('raw_html', sa.Text(), nullable=True))

def downgrade():
    op.drop_column('worklist_items', 'raw_html')
```

#### Part 2: Update Sync Service to Store HTML

```python
# backend/src/services/google_drive/sync_service.py
async def _fetch_document(self, storage, file_id: str, file_metadata: dict) -> dict[str, Any] | None:
    mime_type = file_metadata.get("mimeType")

    if mime_type == "application/vnd.google-apps.document":
        # Export HTML
        html_content = await self._export_google_doc(storage, file_id, "text/html")

        # Parse and clean for backward compatibility
        content, parsing_status = self._parse_html_content(html_content)

        # Parse content with YAML front matter support
        file_name = file_metadata.get("name")
        parsed = self._parse_document_content(content, file_name=file_name)

        # âœ… NEW: Store raw HTML for parser
        parsed["raw_html"] = html_content

        parsed["drive_metadata"] = {
            "id": file_metadata.get("id"),
            "name": file_metadata.get("name"),
            "mimeType": mime_type,
            "webViewLink": file_metadata.get("webViewLink"),
            "createdTime": file_metadata.get("createdTime"),
        }
        return parsed
```

#### Part 3: Update Pipeline to Use Raw HTML

```python
# backend/src/services/worklist/pipeline.py
async def _run_parsing(self, item: WorklistItem) -> bool:
    """Parse document content to extract structured data."""
    try:
        # âœ… Use raw HTML if available, fallback to cleaned content
        raw_html = item.raw_html or item.content

        if not item.raw_html:
            logger.warning(
                "worklist_parsing_no_html",
                worklist_id=item.id,
                message="Using cleaned text as fallback (raw HTML not available)"
            )

        logger.info(
            "worklist_parsing_started",
            worklist_id=item.id,
            content_length=len(raw_html),
            has_raw_html=bool(item.raw_html),
        )

        # Parse with AI (will have images and structure now)
        parsing_result = self.parser_service.parse_document(raw_html)

        # ... rest of implementation ...
```

---

## ğŸ“Š Testing Strategy

### Test Fix #1: Parsing Failure Handling

```python
# tests/services/worklist/test_pipeline_parsing_failure.py
async def test_parsing_failure_stops_proofreading():
    """Verify proofreading doesn't run when parsing fails."""
    # Setup
    pipeline = WorklistPipeline(session, settings)
    item = WorklistItem(id=1, content="test", status=WorklistStatus.PENDING)

    # Mock parser to fail
    mock_parser.parse_document.return_value = ParseResult(
        success=False,
        errors=[ParseError(error_message="AI parsing failed")]
    )

    # Execute
    await pipeline.process_new_item(item)

    # Verify
    assert item.status == WorklistStatus.PARSING  # âœ… Stopped at parsing
    assert item.article.status == ArticleStatus.IMPORTED  # âœ… Not changed
    assert not mock_proofreader.called  # âœ… Proofreading not called

async def test_parsing_exception_stops_proofreading():
    """Verify proofreading doesn't run when parsing throws."""
    # Setup
    pipeline = WorklistPipeline(session, settings)
    item = WorklistItem(id=1, content="test", status=WorklistStatus.PENDING)

    # Mock parser to throw
    mock_parser.parse_document.side_effect = ValueError("Parser crashed")

    # Execute
    await pipeline.process_new_item(item)

    # Verify
    assert item.status == WorklistStatus.PARSING
    assert not mock_proofreader.called

async def test_parsing_success_continues_to_proofreading():
    """Verify proofreading runs when parsing succeeds."""
    # Setup
    pipeline = WorklistPipeline(session, settings)
    item = WorklistItem(id=1, content="test", status=WorklistStatus.PENDING)

    # Mock parser to succeed
    mock_parser.parse_document.return_value = ParseResult(
        success=True,
        parsed_article=ParsedArticle(author_name="Test", images=[...])
    )

    # Execute
    await pipeline.process_new_item(item)

    # Verify
    assert item.status == WorklistStatus.PARSING_REVIEW  # âœ… Parsing done
    # Note: In real flow, human reviews parsing before proofreading
    # This test would need to be adjusted based on workflow decision
```

### Test Fix #2: HTML Storage and Usage

```python
# tests/services/google_drive/test_sync_raw_html.py
async def test_sync_stores_raw_html():
    """Verify raw HTML is stored during sync."""
    # Setup
    mock_html = """
    <html>
        <h1>Test Title</h1>
        <img src="https://example.com/image.jpg">
        <p>Content</p>
    </html>
    """
    mock_storage.export.return_value = mock_html

    # Execute
    result = await sync_service._fetch_document(mock_storage, "file123", metadata)

    # Verify
    assert result["raw_html"] == mock_html  # âœ… HTML preserved
    assert "<img" not in result["content"]  # âœ… Cleaned text also stored

# tests/services/worklist/test_pipeline_html_parsing.py
async def test_parser_receives_html_with_images():
    """Verify parser gets HTML with image tags."""
    # Setup
    html_with_images = '<h1>Title</h1><img src="test.jpg"><p>Content</p>'
    item = WorklistItem(
        id=1,
        content="Title\nContent",  # Cleaned
        raw_html=html_with_images  # Raw HTML
    )

    # Execute
    await pipeline._run_parsing(item)

    # Verify
    mock_parser.parse_document.assert_called_with(html_with_images)  # âœ… Got HTML
    # Verify images were extracted
    assert len(item.drive_metadata["images"]) > 0

async def test_parser_fallback_to_content():
    """Verify parser falls back to cleaned content if no HTML."""
    # Setup
    item = WorklistItem(
        id=1,
        content="Title\nContent",
        raw_html=None  # No HTML available
    )

    # Execute
    await pipeline._run_parsing(item)

    # Verify
    mock_parser.parse_document.assert_called_with("Title\nContent")  # âœ… Used content
```

---

## ğŸ“‹ Implementation Checklist

### Phase 1: Fix Parsing Failure Handling (1-2 hours)
- [ ] Update `_run_parsing` to return `bool`
- [ ] Update `process_new_item` to check return value
- [ ] Add logging for skipped proofreading
- [ ] Write unit tests (3 scenarios)
- [ ] Run existing tests to ensure no regression
- [ ] Manual testing with failing parser

### Phase 2: Add Raw HTML Storage (3-4 hours)
- [ ] Create Alembic migration for `raw_html` column
- [ ] Run migration on dev database
- [ ] Update `WorklistItem` model
- [ ] Update `sync_service._fetch_document` to store HTML
- [ ] Update `pipeline._run_parsing` to use raw HTML
- [ ] Add fallback logic for items without HTML
- [ ] Write integration tests
- [ ] Test with real Google Docs export

### Phase 3: Backfill Existing Items (Optional, 1-2 hours)
- [ ] Create script to re-sync items without `raw_html`
- [ ] Dry-run to verify backfill logic
- [ ] Execute backfill on production
- [ ] Monitor for errors

### Phase 4: Validation & Monitoring (1 hour)
- [ ] End-to-end test with real document
- [ ] Verify images are extracted correctly
- [ ] Add metrics for parsing success/failure rates
- [ ] Add alert for high parsing failure rate
- [ ] Update documentation

---

## ğŸ¯ Expected Outcomes

### After Fix #1
âœ… Parsing failures properly stop the pipeline
âœ… Items stuck at `PARSING` status until manual review
âœ… Proofreading only runs on successfully parsed items
âœ… Operators see clear parsing failure messages
âœ… No more masked failures

### After Fix #2
âœ… Parser receives original HTML with images
âœ… Images successfully extracted from documents
âœ… Structural cues available for heuristic parsing
âœ… AI prompting uses rich HTML context
âœ… Parsing stage meets design requirements

---

## ğŸš¨ Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Migration fails on production | Low | High | Test on staging first, have rollback plan |
| Backfill consumes too many Drive API quotas | Medium | Medium | Rate limit backfill script, run during off-hours |
| Raw HTML column increases DB size significantly | High | Low | HTML is text (compressible), monitor growth |
| Existing items without raw_html fail parsing | High | Medium | Implement fallback to cleaned content |
| Boolean return breaks other callers | Low | High | Check all callers of `_run_parsing` (none found) |

---

## ğŸ“š References

**Issue #1 Analysis**:
- Code: `backend/src/services/worklist/pipeline.py:47-55, 94-210`
- Related: `backend/src/models/worklist.py` (WorklistStatus enum)

**Issue #2 Analysis**:
- Code: `backend/src/services/worklist/pipeline.py:94-109`
- Code: `backend/src/services/google_drive/sync_service.py:256-336, 353-379`
- Related: `backend/src/services/article_parser/service.py` (expects HTML)

---

**Analysis Completed**: 2025-11-12
**Analyst**: Claude (Anthropic) + Codex CLI
**Status**: âœ… Both issues confirmed and reproduction paths validated
**Priority**: ğŸ”´ P1 - Both issues block parsing functionality
**Recommended Action**: Implement fixes in order (Fix #1 first, then Fix #2)
